{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n",
    "\n",
    "In this notebook, you will implement a deep Q-Learning reinforcement algorithm. The implementation borrows ideas from both the original DeepMind Nature paper and the more recent asynchronous version:<br/>\n",
    "[1] \"Human-Level Control through Deep Reinforcement Learning\" by Mnih et al. 2015<br/>\n",
    "[2] \"Asynchronous Methods for Deep Reinforcement Learning\" by Mnih et al. 2016.<br/>\n",
    "\n",
    "In particular:\n",
    "* We use separate target and Q-functions estimators with periodic updates to the target estimator. \n",
    "* We use several concurrent \"threads\" rather than experience replay to generate less biased gradient updates. \n",
    "* Threads are actually synchronized so we start each one at a random number of moves.\n",
    "* We use an epsilon-greedy policy that blends random moves with policy moves.\n",
    "* We taper the random action parameter (epsilon) and the learning rate to zero during training.\n",
    "\n",
    "This gives a simple and reasonably fast general-purpose RL algorithm. We use it here for the Cartpole environment from OpenAI Gym, but it can easily be adapted to others. For this notebook, you will implement 4 steps:\n",
    "\n",
    "1. The backward step for the Q-estimator\n",
    "2. The $\\epsilon$-greedy policy\n",
    "3. \"asynchronous\" initialization \n",
    "4. The Q-learning algorithm\n",
    "\n",
    "To get started, we import some prerequisites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajagrawal/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below lists some parameters you can tune. They should be self-explanatory. They are currently set to train CartPole-V0 to a \"solved\" score (> 195) most of the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partners:  Ashwin P., Kevin L. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nsteps = 10001                       # Number of steps to run (game actions per environment)\n",
    "npar = 16                            # Number of parallel environments\n",
    "target_window = 200                  # Interval to update target estimator from q-estimator\n",
    "discount_factor = 0.99               # Reward discount factor\n",
    "printsteps = 1000                    # Number of steps between printouts\n",
    "render = False                       # Whether to render an environment while training\n",
    "\n",
    "epsilon_start = 1.0                  # Parameters for epsilon-greedy policy: initial epsilon\n",
    "epsilon_end = 0.0                    # Final epsilon\n",
    "neps = int(0.8*nsteps)               # Number of steps to decay epsilon\n",
    "\n",
    "learning_rate = 2e-3                 # Initial learning rate\n",
    "lr_end = 0                           # Final learning rate\n",
    "nlr = neps                           # Steps to decay learning rate\n",
    "decay_rate = 0.99                    # Decay factor for RMSProp \n",
    "\n",
    "nhidden = 200                        # Number of hidden layers for estimators\n",
    "\n",
    "init_moves = 2000                    # Upper bound on random number of moves to take initially\n",
    "nwindow = 2                          # Sensing window = last n images in a state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are environment-specific parameters. The function \"preprocess\" should process an observation returned by the environment into a vector for training. For CartPole we simply append a 1 to implement bias in the first layer. \n",
    "\n",
    "For visual environments you would typically crop, downsample to 80x80, set color to a single bit (foreground/background), and flatten to a vector. That transformation is already implemented in the Policy Gradient code.\n",
    "\n",
    "*nfeats* is the dimension of the vector output by *preprocess*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "game_type=\"CartPole-v0\"                 # Model type and action definitions\n",
    "VALID_ACTIONS = [0, 1]\n",
    "nactions = len(VALID_ACTIONS)\n",
    "nfeats = 5                              # There are four state features plus the constant we add\n",
    "\n",
    "def preprocess(I):                      # preprocess each observation\n",
    "    \"\"\"Just append a 1 to the end\"\"\"\n",
    "    return np.append(I.astype(float),1) # Add a constant feature for bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Q-estimator class. We use two instances of this class, one for the target estimator, and one for the Q-estimator. The Q function is normally represented as a scalar $Q(x,a)$ where $x$ is the state and $a$ is an action. For ease of implementation, we actually estimate a vector-valued function $Q(x,.)$ which returns the estimated reward for every action. The model here has just a single hidden layer:\n",
    "\n",
    "<pre>\n",
    "Input Layer (nfeats) => FC Layer => RELU => FC Layer => Output (naction values)\n",
    "</pre>\n",
    "\n",
    "## 1. Implement Q-estimator gradient\n",
    "Your first task is to implement the\n",
    "<pre>Estimator.gradient(s, a, y)</pre>\n",
    "method for this class. **gradient** should compute the gradients wrt weight arrays W1 and W2 into\n",
    "<pre>self.grad['W1']\n",
    "self.grad['W2']</pre>\n",
    "respectively. Both <code>a</code> and <code>y</code> are vectors. Be sure to update only the output layer weights corresponding to the given action vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "\n",
    "    def __init__(self, ninputs, nhidden, nactions):\n",
    "        \"\"\" Create model matrices, and gradient and squared gradient buffers\"\"\"\n",
    "        model = {}\n",
    "        model['W1'] = np.random.randn(nhidden, ninputs) / np.sqrt(ninputs)   # \"Xavier\" initialization\n",
    "        model['W2'] = np.random.randn(nactions, nhidden) / np.sqrt(nhidden)\n",
    "        self.model = model\n",
    "        self.grad = { k : np.zeros_like(v) for k,v in model.iteritems() }\n",
    "        self.gradsq = { k : np.zeros_like(v) for k,v in model.iteritems() }   \n",
    "        \n",
    "\n",
    "    def forward(self, s):\n",
    "        \"\"\" Run the model forward given a state as input.\n",
    "    returns action predictions and the hidden state\"\"\"\n",
    "        h = np.dot(self.model['W1'], s)\n",
    "        h[h<0] = 0 # ReLU nonlinearity\n",
    "        rew = np.dot(self.model['W2'], h)\n",
    "        return rew, h\n",
    "    \n",
    "    \n",
    "    def predict(self, s):\n",
    "        \"\"\" Predict the action rewards from a given input state\"\"\"\n",
    "        rew, h = self.forward(s)\n",
    "        return rew\n",
    "    \n",
    "              \n",
    "    def gradient(self, s, a, y):\n",
    "        \"\"\" Given a state s, action a and target y, compute the model gradients\"\"\"\n",
    "    ##################################################################################\n",
    "    ##                                                                              ##\n",
    "    ## TODO: Compute gradients and return a scalar loss on a minibatch of size npar ##\n",
    "    ##    s is the input state matrix (ninputs x npar).                             ##\n",
    "    ##    a is an action vector (npar,).                                            ##\n",
    "    ##    y is a vector of target values (npar,) corresponding to those actions.    ##\n",
    "    ##    return: the loss per sample (npar,).                                      ##                          \n",
    "    ##                                                                              ##\n",
    "    ## Notes:                                                                       ##\n",
    "    ##    * If the action is ai in [0,...,nactions-1], backprop only through the    ##\n",
    "    ##      ai'th output.                                                           ##\n",
    "    ##    * loss should be L2, and we recommend you normalize it to a per-input     ##\n",
    "    ##      value, i.e. return L2(target,predition)/sqrt(npar).                     ##\n",
    "    ##    * save the gradients in self.grad['W1'] and self.grad['W2'].              ##\n",
    "    ##    * update self.grad['W1'] and self.grad['W2'] by adding the gradients, so  ##\n",
    "    ##      that multiple gradient steps can be used beteween updates.              ##\n",
    "    ##                                                                              ##\n",
    "    ##################################################################################\n",
    "        loss = 0.0 \n",
    "        prediction, h = self.forward(s)\n",
    "        diffs = np.zeros(npar)\n",
    "        dW2 = np.zeros_like(self.model['W2'])\n",
    "        dW1 = np.zeros_like(self.model['W1'])\n",
    "        num_hidden = h.shape[0]\n",
    "        for i in range(npar):\n",
    "            si = s[:, i]\n",
    "            ai = a[i]\n",
    "            hi = h[:, i]\n",
    "            pred_i = prediction[ai, i]\n",
    "            residual = pred_i - y[i]\n",
    "            diffs[i] = residual \n",
    "            dW2[ai, :] += 2 * residual * hi \n",
    "            dhidden = 2 * residual * self.model['W2'][ai, :]\n",
    "            dhidden[hi == 0] = 0 \n",
    "            dW1 += np.outer(dhidden, si)\n",
    "        \n",
    "        loss = np.sum(diffs ** 2) / npar\n",
    "        dW1 /= npar\n",
    "        dW2 /= npar\n",
    "        \n",
    "        self.grad['W1'] = dW1\n",
    "        self.grad['W2'] = dW2\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def rmsprop(self, learning_rate, decay_rate): \n",
    "        \"\"\" Perform model updates from the gradients using RMSprop\"\"\"\n",
    "        for k in self.model:\n",
    "            g = self.grad[k]\n",
    "            self.gradsq[k] = decay_rate * self.gradsq[k] + (1 - decay_rate) * g*g\n",
    "            self.model[k] -= learning_rate * g / (np.sqrt(self.gradsq[k]) + 1e-5)\n",
    "            self.grad[k].fill(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # npar = 2\n",
    "# check_est = Estimator(3, 8, 2)\n",
    "# s = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32).T\n",
    "# action = [0, 1]\n",
    "# y = [2, 3]\n",
    "# check_est.gradient(s, action, y)\n",
    "# #print(check_est.predict(s))\n",
    "# print(check_est.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement $\\epsilon$-Greedy Policy\n",
    "\n",
    "An $\\epsilon$-Greedy policy should:\n",
    "* with probability $\\epsilon$ take a uniformly-random action.\n",
    "* otherwise choose the best action according to the estimator from the given state.\n",
    "\n",
    "The function below should implement this policy. It should return a matrix A of size (nactions, npar) such that A[i,j] is the probability of taking action i on input j. The probabilities of non-optimal actions should be $\\epsilon/{\\rm nactions}$ and the probability of the best action should be $1-\\epsilon+\\epsilon/{\\rm nactions}$.\n",
    "\n",
    "Since the function processes batches of states, the input <code>state</code> is a <code>ninputs x npar</code> matrix, and the returned value should be a <code>nactions x npar</code> matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy(estimator, state, epsilon):\n",
    "    \"\"\" Take an estimator and state and predict the best action.\n",
    "    For each input state, return a vector of action probabilities according to an epsilon-greedy policy\"\"\"\n",
    "    ##################################################################################\n",
    "    ##                                                                              ##\n",
    "    ## TODO: Implement an epsilon-greedy policy                                     ##\n",
    "    ##       estimator: is the estimator to use (instance of Estimator)             ##\n",
    "    ##       state is an (ninputs x npar) state matrix                              ##\n",
    "    ##       epsilon is the scalar policy parameter                                 ##\n",
    "    ## return: an (nactions x npar) matrix A where A[i,j] is the probability of     ##\n",
    "    ##       taking action i on input j.                                            ##\n",
    "    ##                                                                              ##\n",
    "    ## Use the definition of epsilon-greedy from the cell above.                    ##\n",
    "    ##                                                                              ##\n",
    "    ##################################################################################\n",
    "    predictions = estimator.predict(state)\n",
    "    nactions, npar = predictions.shape\n",
    "    best_action = np.argmax(predictions, axis=0)\n",
    "    A = np.zeros((nactions, npar))\n",
    "    A += epsilon/nactions\n",
    "    for i in range(npar):\n",
    "        A[best_action[i], i] = 1 - epsilon + (epsilon / nactions)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This routine copies the state of one estimator into another. Its used to update the target estimator from the Q-estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_estimator(to_estimator, from_estimator, window, istep):\n",
    "    \"\"\" every <window> steps, Copy model state from from_estimator into to_estimator\"\"\"\n",
    "    if (istep % window == 0):\n",
    "        for k in from_estimator.model:\n",
    "            np.copyto(to_estimator.model[k], from_estimator.model[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement \"Asynchronous Threads\"\n",
    "\n",
    "Don't try that in Python!! Actually all we do here is create an array of environments and advance each one a random number of steps, using random actions at each step. Later on we will make *synchronous* updates to all the environments, but the environments (and their gradient updates) should remain uncorrelated. This serves the same goal as asynchronous updates in paper [2], or experience replay in paper [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-22 23:48:07,089] Making new env: CartPole-v0\n",
      "[2016-11-22 23:48:07,216] Making new env: CartPole-v0\n",
      "[2016-11-22 23:48:07,251] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 16 timesteps\n",
      "Episode finished after 86 timesteps\n",
      "Episode finished after 100 timesteps\n",
      "Episode finished after 119 timesteps\n",
      "Episode finished after 131 timesteps\n",
      "Episode finished after 19 timesteps\n",
      "Episode finished after 37 timesteps\n",
      "Episode finished after 57 timesteps\n",
      "Episode finished after 76 timesteps\n",
      "Episode finished after 94 timesteps\n",
      "Episode finished after 132 timesteps\n",
      "Episode finished after 162 timesteps\n",
      "Episode finished after 170 timesteps\n",
      "Episode finished after 211 timesteps\n",
      "Episode finished after 241 timesteps\n",
      "Episode finished after 259 timesteps\n",
      "Episode finished after 293 timesteps\n",
      "Episode finished after 310 timesteps\n",
      "Episode finished after 353 timesteps\n",
      "Episode finished after 389 timesteps\n",
      "Episode finished after 403 timesteps\n",
      "Episode finished after 418 timesteps\n",
      "Episode finished after 433 timesteps\n",
      "Episode finished after 448 timesteps\n",
      "Episode finished after 470 timesteps\n",
      "Episode finished after 491 timesteps\n",
      "Episode finished after 506 timesteps\n",
      "Episode finished after 521 timesteps\n",
      "Episode finished after 535 timesteps\n",
      "Episode finished after 566 timesteps\n",
      "Episode finished after 588 timesteps\n",
      "Episode finished after 601 timesteps\n",
      "Episode finished after 635 timesteps\n",
      "Episode finished after 653 timesteps\n",
      "Episode finished after 677 timesteps\n",
      "Episode finished after 688 timesteps\n",
      "Episode finished after 701 timesteps\n",
      "Episode finished after 711 timesteps\n",
      "Episode finished after 723 timesteps\n",
      "Episode finished after 746 timesteps\n",
      "Episode finished after 757 timesteps\n",
      "Episode finished after 769 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 38 timesteps\n",
      "Episode finished after 53 timesteps\n",
      "Episode finished after 71 timesteps\n",
      "Episode finished after 85 timesteps\n",
      "Episode finished after 133 timesteps\n",
      "Episode finished after 149 timesteps\n",
      "Episode finished after 167 timesteps\n",
      "Episode finished after 178 timesteps\n",
      "Episode finished after 201 timesteps\n",
      "Episode finished after 231 timesteps\n",
      "Episode finished after 250 timesteps\n",
      "Episode finished after 275 timesteps\n",
      "Episode finished after 308 timesteps\n",
      "Episode finished after 357 timesteps\n",
      "Episode finished after 375 timesteps\n",
      "Episode finished after 390 timesteps\n",
      "Episode finished after 416 timesteps\n",
      "Episode finished after 454 timesteps\n",
      "Episode finished after 468 timesteps\n",
      "Episode finished after 494 timesteps\n",
      "Episode finished after 510 timesteps\n",
      "Episode finished after 522 timesteps\n",
      "Episode finished after 534 timesteps\n",
      "Episode finished after 552 timesteps\n",
      "Episode finished after 564 timesteps\n",
      "Episode finished after 581 timesteps\n",
      "Episode finished after 614 timesteps\n",
      "Episode finished after 635 timesteps\n",
      "Episode finished after 647 timesteps\n",
      "Episode finished after 659 timesteps\n",
      "Episode finished after 675 timesteps\n",
      "Episode finished after 702 timesteps\n",
      "Episode finished after 716 timesteps\n",
      "Episode finished after 739 timesteps\n",
      "Episode finished after 765 timesteps\n",
      "Episode finished after 795 timesteps\n",
      "Episode finished after 805 timesteps\n",
      "Episode finished after 824 timesteps\n",
      "Episode finished after 865 timesteps\n",
      "Episode finished after 884 timesteps\n",
      "Episode finished after 901 timesteps\n",
      "Episode finished after 928 timesteps\n",
      "Episode finished after 938 timesteps\n",
      "Episode finished after 949 timesteps\n",
      "Episode finished after 972 timesteps\n",
      "Episode finished after 983 timesteps\n",
      "Episode finished after 1006 timesteps\n",
      "Episode finished after 1020 timesteps\n",
      "Episode finished after 1050 timesteps\n",
      "Episode finished after 1095 timesteps\n",
      "Episode finished after 1119 timesteps\n",
      "Episode finished after 1131 timesteps\n",
      "Episode finished after 1156 timesteps\n",
      "Episode finished after 1182 timesteps\n",
      "Episode finished after 1222 timesteps\n",
      "Episode finished after 1234 timesteps\n",
      "Episode finished after 1250 timesteps\n",
      "Episode finished after 1262 timesteps\n",
      "Episode finished after 1276 timesteps\n",
      "Episode finished after 1309 timesteps\n",
      "Episode finished after 1328 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-22 23:48:07,326] Making new env: CartPole-v0\n",
      "[2016-11-22 23:48:07,343] Making new env: CartPole-v0\n",
      "[2016-11-22 23:48:07,397] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1385 timesteps\n",
      "Episode finished after 1394 timesteps\n",
      "Episode finished after 1408 timesteps\n",
      "Episode finished after 1429 timesteps\n",
      "Episode finished after 1448 timesteps\n",
      "Episode finished after 1462 timesteps\n",
      "Episode finished after 1472 timesteps\n",
      "Episode finished after 1501 timesteps\n",
      "Episode finished after 1516 timesteps\n",
      "Episode finished after 1569 timesteps\n",
      "Episode finished after 1580 timesteps\n",
      "Episode finished after 1591 timesteps\n",
      "Episode finished after 1630 timesteps\n",
      "Episode finished after 1647 timesteps\n",
      "Episode finished after 1667 timesteps\n",
      "Episode finished after 1678 timesteps\n",
      "Episode finished after 1691 timesteps\n",
      "Episode finished after 1720 timesteps\n",
      "Episode finished after 1735 timesteps\n",
      "Episode finished after 1768 timesteps\n",
      "Episode finished after 18 timesteps\n",
      "Episode finished after 35 timesteps\n",
      "Episode finished after 60 timesteps\n",
      "Episode finished after 74 timesteps\n",
      "Episode finished after 146 timesteps\n",
      "Episode finished after 185 timesteps\n",
      "Episode finished after 211 timesteps\n",
      "Episode finished after 225 timesteps\n",
      "Episode finished after 239 timesteps\n",
      "Episode finished after 254 timesteps\n",
      "Episode finished after 276 timesteps\n",
      "Episode finished after 289 timesteps\n",
      "Episode finished after 342 timesteps\n",
      "Episode finished after 356 timesteps\n",
      "Episode finished after 367 timesteps\n",
      "Episode finished after 424 timesteps\n",
      "Episode finished after 442 timesteps\n",
      "Episode finished after 454 timesteps\n",
      "Episode finished after 465 timesteps\n",
      "Episode finished after 489 timesteps\n",
      "Episode finished after 527 timesteps\n",
      "Episode finished after 540 timesteps\n",
      "Episode finished after 557 timesteps\n",
      "Episode finished after 572 timesteps\n",
      "Episode finished after 581 timesteps\n",
      "Episode finished after 594 timesteps\n",
      "Episode finished after 607 timesteps\n",
      "Episode finished after 618 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 21 timesteps\n",
      "Episode finished after 30 timesteps\n",
      "Episode finished after 42 timesteps\n",
      "Episode finished after 60 timesteps\n",
      "Episode finished after 91 timesteps\n",
      "Episode finished after 105 timesteps\n",
      "Episode finished after 121 timesteps\n",
      "Episode finished after 141 timesteps\n",
      "Episode finished after 156 timesteps\n",
      "Episode finished after 188 timesteps\n",
      "Episode finished after 207 timesteps\n",
      "Episode finished after 227 timesteps\n",
      "Episode finished after 255 timesteps\n",
      "Episode finished after 270 timesteps\n",
      "Episode finished after 302 timesteps\n",
      "Episode finished after 322 timesteps\n",
      "Episode finished after 331 timesteps\n",
      "Episode finished after 367 timesteps\n",
      "Episode finished after 382 timesteps\n",
      "Episode finished after 403 timesteps\n",
      "Episode finished after 425 timesteps\n",
      "Episode finished after 461 timesteps\n",
      "Episode finished after 487 timesteps\n",
      "Episode finished after 509 timesteps\n",
      "Episode finished after 519 timesteps\n",
      "Episode finished after 540 timesteps\n",
      "Episode finished after 556 timesteps\n",
      "Episode finished after 606 timesteps\n",
      "Episode finished after 638 timesteps\n",
      "Episode finished after 654 timesteps\n",
      "Episode finished after 688 timesteps\n",
      "Episode finished after 720 timesteps\n",
      "Episode finished after 730 timesteps\n",
      "Episode finished after 753 timesteps\n",
      "Episode finished after 768 timesteps\n",
      "Episode finished after 777 timesteps\n",
      "Episode finished after 792 timesteps\n",
      "Episode finished after 813 timesteps\n",
      "Episode finished after 845 timesteps\n",
      "Episode finished after 857 timesteps\n",
      "Episode finished after 871 timesteps\n",
      "Episode finished after 898 timesteps\n",
      "Episode finished after 912 timesteps\n",
      "Episode finished after 927 timesteps\n",
      "Episode finished after 947 timesteps\n",
      "Episode finished after 978 timesteps\n",
      "Episode finished after 1009 timesteps\n",
      "Episode finished after 1023 timesteps\n",
      "Episode finished after 1044 timesteps\n",
      "Episode finished after 1057 timesteps\n",
      "Episode finished after 1074 timesteps\n",
      "Episode finished after 1086 timesteps\n",
      "Episode finished after 1095 timesteps\n",
      "Episode finished after 1108 timesteps\n",
      "Episode finished after 1224 timesteps\n",
      "Episode finished after 1239 timesteps\n",
      "Episode finished after 1253 timesteps\n",
      "Episode finished after 1276 timesteps\n",
      "Episode finished after 1300 timesteps\n",
      "Episode finished after 1333 timesteps\n",
      "Episode finished after 1348 timesteps\n",
      "Episode finished after 1379 timesteps\n",
      "Episode finished after 1428 timesteps\n",
      "Episode finished after 1453 timesteps\n",
      "Episode finished after 1483 timesteps\n",
      "Episode finished after 1497 timesteps\n",
      "Episode finished after 14 timesteps\n",
      "Episode finished after 31 timesteps\n",
      "Episode finished after 53 timesteps\n",
      "Episode finished after 65 timesteps\n",
      "Episode finished after 91 timesteps\n",
      "Episode finished after 119 timesteps\n",
      "Episode finished after 131 timesteps\n",
      "Episode finished after 147 timesteps\n",
      "Episode finished after 164 timesteps\n",
      "Episode finished after 179 timesteps\n",
      "Episode finished after 193 timesteps\n",
      "Episode finished after 205 timesteps\n",
      "Episode finished after 270 timesteps\n",
      "Episode finished after 308 timesteps\n",
      "Episode finished after 331 timesteps\n",
      "Episode finished after 348 timesteps\n",
      "Episode finished after 386 timesteps\n",
      "Episode finished after 397 timesteps\n",
      "Episode finished after 414 timesteps\n",
      "Episode finished after 434 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-22 23:48:07,454] Making new env: CartPole-v0\n",
      "[2016-11-22 23:48:07,479] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 476 timesteps\n",
      "Episode finished after 505 timesteps\n",
      "Episode finished after 513 timesteps\n",
      "Episode finished after 531 timesteps\n",
      "Episode finished after 544 timesteps\n",
      "Episode finished after 567 timesteps\n",
      "Episode finished after 579 timesteps\n",
      "Episode finished after 588 timesteps\n",
      "Episode finished after 600 timesteps\n",
      "Episode finished after 613 timesteps\n",
      "Episode finished after 656 timesteps\n",
      "Episode finished after 678 timesteps\n",
      "Episode finished after 727 timesteps\n",
      "Episode finished after 762 timesteps\n",
      "Episode finished after 771 timesteps\n",
      "Episode finished after 791 timesteps\n",
      "Episode finished after 804 timesteps\n",
      "Episode finished after 819 timesteps\n",
      "Episode finished after 842 timesteps\n",
      "Episode finished after 854 timesteps\n",
      "Episode finished after 879 timesteps\n",
      "Episode finished after 902 timesteps\n",
      "Episode finished after 924 timesteps\n",
      "Episode finished after 943 timesteps\n",
      "Episode finished after 967 timesteps\n",
      "Episode finished after 994 timesteps\n",
      "Episode finished after 1042 timesteps\n",
      "Episode finished after 1056 timesteps\n",
      "Episode finished after 1088 timesteps\n",
      "Episode finished after 1114 timesteps\n",
      "Episode finished after 1129 timesteps\n",
      "Episode finished after 1138 timesteps\n",
      "Episode finished after 1152 timesteps\n",
      "Episode finished after 1164 timesteps\n",
      "Episode finished after 1206 timesteps\n",
      "Episode finished after 1229 timesteps\n",
      "Episode finished after 1244 timesteps\n",
      "Episode finished after 1273 timesteps\n",
      "Episode finished after 1284 timesteps\n",
      "Episode finished after 1306 timesteps\n",
      "Episode finished after 1328 timesteps\n",
      "Episode finished after 1353 timesteps\n",
      "Episode finished after 17 timesteps\n",
      "Episode finished after 27 timesteps\n",
      "Episode finished after 39 timesteps\n",
      "Episode finished after 55 timesteps\n",
      "Episode finished after 86 timesteps\n",
      "Episode finished after 126 timesteps\n",
      "Episode finished after 158 timesteps\n",
      "Episode finished after 174 timesteps\n",
      "Episode finished after 187 timesteps\n",
      "Episode finished after 202 timesteps\n",
      "Episode finished after 232 timesteps\n",
      "Episode finished after 260 timesteps\n",
      "Episode finished after 278 timesteps\n",
      "Episode finished after 288 timesteps\n",
      "Episode finished after 314 timesteps\n",
      "Episode finished after 348 timesteps\n",
      "Episode finished after 358 timesteps\n",
      "Episode finished after 378 timesteps\n",
      "Episode finished after 402 timesteps\n",
      "Episode finished after 414 timesteps\n",
      "Episode finished after 435 timesteps\n",
      "Episode finished after 452 timesteps\n",
      "Episode finished after 469 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 14 timesteps\n",
      "Episode finished after 52 timesteps\n",
      "Episode finished after 88 timesteps\n",
      "Episode finished after 101 timesteps\n",
      "Episode finished after 127 timesteps\n",
      "Episode finished after 140 timesteps\n",
      "Episode finished after 168 timesteps\n",
      "Episode finished after 185 timesteps\n",
      "Episode finished after 213 timesteps\n",
      "Episode finished after 228 timesteps\n",
      "Episode finished after 248 timesteps\n",
      "Episode finished after 277 timesteps\n",
      "Episode finished after 293 timesteps\n",
      "Episode finished after 307 timesteps\n",
      "Episode finished after 324 timesteps\n",
      "Episode finished after 340 timesteps\n",
      "Episode finished after 352 timesteps\n",
      "Episode finished after 364 timesteps\n",
      "Episode finished after 375 timesteps\n",
      "Episode finished after 392 timesteps\n",
      "Episode finished after 410 timesteps\n",
      "Episode finished after 420 timesteps\n",
      "Episode finished after 438 timesteps\n",
      "Episode finished after 448 timesteps\n",
      "Episode finished after 461 timesteps\n",
      "Episode finished after 477 timesteps\n",
      "Episode finished after 496 timesteps\n",
      "Episode finished after 514 timesteps\n",
      "Episode finished after 534 timesteps\n",
      "Episode finished after 547 timesteps\n",
      "Episode finished after 569 timesteps\n",
      "Episode finished after 586 timesteps\n",
      "Episode finished after 597 timesteps\n",
      "Episode finished after 614 timesteps\n",
      "Episode finished after 626 timesteps\n",
      "Episode finished after 657 timesteps\n",
      "Episode finished after 681 timesteps\n",
      "Episode finished after 691 timesteps\n",
      "Episode finished after 725 timesteps\n",
      "Episode finished after 736 timesteps\n",
      "Episode finished after 747 timesteps\n",
      "Episode finished after 758 timesteps\n",
      "Episode finished after 779 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-22 23:48:07,535] Making new env: CartPole-v0\n",
      "[2016-11-22 23:48:07,613] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 801 timesteps\n",
      "Episode finished after 839 timesteps\n",
      "Episode finished after 881 timesteps\n",
      "Episode finished after 897 timesteps\n",
      "Episode finished after 938 timesteps\n",
      "Episode finished after 989 timesteps\n",
      "Episode finished after 1001 timesteps\n",
      "Episode finished after 1023 timesteps\n",
      "Episode finished after 1039 timesteps\n",
      "Episode finished after 1067 timesteps\n",
      "Episode finished after 1089 timesteps\n",
      "Episode finished after 1105 timesteps\n",
      "Episode finished after 1117 timesteps\n",
      "Episode finished after 1135 timesteps\n",
      "Episode finished after 1151 timesteps\n",
      "Episode finished after 1165 timesteps\n",
      "Episode finished after 1199 timesteps\n",
      "Episode finished after 1218 timesteps\n",
      "Episode finished after 1231 timesteps\n",
      "Episode finished after 1245 timesteps\n",
      "Episode finished after 1263 timesteps\n",
      "Episode finished after 1287 timesteps\n",
      "Episode finished after 1314 timesteps\n",
      "Episode finished after 1326 timesteps\n",
      "Episode finished after 15 timesteps\n",
      "Episode finished after 31 timesteps\n",
      "Episode finished after 43 timesteps\n",
      "Episode finished after 60 timesteps\n",
      "Episode finished after 73 timesteps\n",
      "Episode finished after 87 timesteps\n",
      "Episode finished after 122 timesteps\n",
      "Episode finished after 134 timesteps\n",
      "Episode finished after 203 timesteps\n",
      "Episode finished after 215 timesteps\n",
      "Episode finished after 269 timesteps\n",
      "Episode finished after 285 timesteps\n",
      "Episode finished after 302 timesteps\n",
      "Episode finished after 317 timesteps\n",
      "Episode finished after 341 timesteps\n",
      "Episode finished after 352 timesteps\n",
      "Episode finished after 372 timesteps\n",
      "Episode finished after 386 timesteps\n",
      "Episode finished after 422 timesteps\n",
      "Episode finished after 451 timesteps\n",
      "Episode finished after 463 timesteps\n",
      "Episode finished after 480 timesteps\n",
      "Episode finished after 501 timesteps\n",
      "Episode finished after 518 timesteps\n",
      "Episode finished after 528 timesteps\n",
      "Episode finished after 595 timesteps\n",
      "Episode finished after 645 timesteps\n",
      "Episode finished after 664 timesteps\n",
      "Episode finished after 675 timesteps\n",
      "Episode finished after 685 timesteps\n",
      "Episode finished after 700 timesteps\n",
      "Episode finished after 712 timesteps\n",
      "Episode finished after 724 timesteps\n",
      "Episode finished after 738 timesteps\n",
      "Episode finished after 759 timesteps\n",
      "Episode finished after 786 timesteps\n",
      "Episode finished after 843 timesteps\n",
      "Episode finished after 856 timesteps\n",
      "Episode finished after 869 timesteps\n",
      "Episode finished after 879 timesteps\n",
      "Episode finished after 890 timesteps\n",
      "Episode finished after 922 timesteps\n",
      "Episode finished after 936 timesteps\n",
      "Episode finished after 982 timesteps\n",
      "Episode finished after 1000 timesteps\n",
      "Episode finished after 1018 timesteps\n",
      "Episode finished after 1041 timesteps\n",
      "Episode finished after 1057 timesteps\n",
      "Episode finished after 1070 timesteps\n",
      "Episode finished after 1088 timesteps\n",
      "Episode finished after 1107 timesteps\n",
      "Episode finished after 1134 timesteps\n",
      "Episode finished after 1149 timesteps\n",
      "Episode finished after 1174 timesteps\n",
      "Episode finished after 1192 timesteps\n",
      "Episode finished after 1205 timesteps\n",
      "Episode finished after 1215 timesteps\n",
      "Episode finished after 1228 timesteps\n",
      "Episode finished after 1310 timesteps\n",
      "Episode finished after 1337 timesteps\n",
      "Episode finished after 1387 timesteps\n",
      "Episode finished after 1413 timesteps\n",
      "Episode finished after 1435 timesteps\n",
      "Episode finished after 1452 timesteps\n",
      "Episode finished after 1469 timesteps\n",
      "Episode finished after 1483 timesteps\n",
      "Episode finished after 1498 timesteps\n",
      "Episode finished after 1524 timesteps\n",
      "Episode finished after 1544 timesteps\n",
      "Episode finished after 1573 timesteps\n",
      "Episode finished after 1595 timesteps\n",
      "Episode finished after 1611 timesteps\n",
      "Episode finished after 1636 timesteps\n",
      "Episode finished after 9 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-22 23:48:07,633] Making new env: CartPole-v0\n",
      "[2016-11-22 23:48:07,665] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 21 timesteps\n",
      "Episode finished after 47 timesteps\n",
      "Episode finished after 61 timesteps\n",
      "Episode finished after 72 timesteps\n",
      "Episode finished after 87 timesteps\n",
      "Episode finished after 112 timesteps\n",
      "Episode finished after 131 timesteps\n",
      "Episode finished after 13 timesteps\n",
      "Episode finished after 41 timesteps\n",
      "Episode finished after 52 timesteps\n",
      "Episode finished after 78 timesteps\n",
      "Episode finished after 106 timesteps\n",
      "Episode finished after 137 timesteps\n",
      "Episode finished after 188 timesteps\n",
      "Episode finished after 270 timesteps\n",
      "Episode finished after 284 timesteps\n",
      "Episode finished after 298 timesteps\n",
      "Episode finished after 313 timesteps\n",
      "Episode finished after 332 timesteps\n",
      "Episode finished after 348 timesteps\n",
      "Episode finished after 366 timesteps\n",
      "Episode finished after 380 timesteps\n",
      "Episode finished after 390 timesteps\n",
      "Episode finished after 404 timesteps\n",
      "Episode finished after 428 timesteps\n",
      "Episode finished after 451 timesteps\n",
      "Episode finished after 473 timesteps\n",
      "Episode finished after 501 timesteps\n",
      "Episode finished after 529 timesteps\n",
      "Episode finished after 545 timesteps\n",
      "Episode finished after 18 timesteps\n",
      "Episode finished after 86 timesteps\n",
      "Episode finished after 110 timesteps\n",
      "Episode finished after 142 timesteps\n",
      "Episode finished after 178 timesteps\n",
      "Episode finished after 195 timesteps\n",
      "Episode finished after 213 timesteps\n",
      "Episode finished after 244 timesteps\n",
      "Episode finished after 264 timesteps\n",
      "Episode finished after 282 timesteps\n",
      "Episode finished after 320 timesteps\n",
      "Episode finished after 337 timesteps\n",
      "Episode finished after 358 timesteps\n",
      "Episode finished after 398 timesteps\n",
      "Episode finished after 409 timesteps\n",
      "Episode finished after 422 timesteps\n",
      "Episode finished after 436 timesteps\n",
      "Episode finished after 452 timesteps\n",
      "Episode finished after 501 timesteps\n",
      "Episode finished after 518 timesteps\n",
      "Episode finished after 544 timesteps\n",
      "Episode finished after 570 timesteps\n",
      "Episode finished after 608 timesteps\n",
      "Episode finished after 622 timesteps\n",
      "Episode finished after 640 timesteps\n",
      "Episode finished after 656 timesteps\n",
      "Episode finished after 689 timesteps\n",
      "Episode finished after 714 timesteps\n",
      "Episode finished after 735 timesteps\n",
      "Episode finished after 747 timesteps\n",
      "Episode finished after 775 timesteps\n",
      "Episode finished after 795 timesteps\n",
      "Episode finished after 834 timesteps\n",
      "Episode finished after 853 timesteps\n",
      "Episode finished after 872 timesteps\n",
      "Episode finished after 922 timesteps\n",
      "Episode finished after 935 timesteps\n",
      "Episode finished after 966 timesteps\n",
      "Episode finished after 975 timesteps\n",
      "Episode finished after 993 timesteps\n",
      "Episode finished after 1004 timesteps\n",
      "Episode finished after 1021 timesteps\n",
      "Episode finished after 1041 timesteps\n",
      "Episode finished after 1079 timesteps\n",
      "Episode finished after 1111 timesteps\n",
      "Episode finished after 1127 timesteps\n",
      "Episode finished after 1155 timesteps\n",
      "Episode finished after 1183 timesteps\n",
      "Episode finished after 1194 timesteps\n",
      "Episode finished after 1218 timesteps\n",
      "Episode finished after 1231 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-22 23:48:07,722] Making new env: CartPole-v0\n",
      "[2016-11-22 23:48:07,732] Making new env: CartPole-v0\n",
      "[2016-11-22 23:48:07,796] Making new env: CartPole-v0\n",
      "[2016-11-22 23:48:07,811] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1265 timesteps\n",
      "Episode finished after 1279 timesteps\n",
      "Episode finished after 1298 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 29 timesteps\n",
      "Episode finished after 45 timesteps\n",
      "Episode finished after 78 timesteps\n",
      "Episode finished after 96 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 22 timesteps\n",
      "Episode finished after 35 timesteps\n",
      "Episode finished after 53 timesteps\n",
      "Episode finished after 92 timesteps\n",
      "Episode finished after 108 timesteps\n",
      "Episode finished after 126 timesteps\n",
      "Episode finished after 144 timesteps\n",
      "Episode finished after 163 timesteps\n",
      "Episode finished after 189 timesteps\n",
      "Episode finished after 204 timesteps\n",
      "Episode finished after 242 timesteps\n",
      "Episode finished after 262 timesteps\n",
      "Episode finished after 291 timesteps\n",
      "Episode finished after 305 timesteps\n",
      "Episode finished after 326 timesteps\n",
      "Episode finished after 350 timesteps\n",
      "Episode finished after 367 timesteps\n",
      "Episode finished after 388 timesteps\n",
      "Episode finished after 404 timesteps\n",
      "Episode finished after 425 timesteps\n",
      "Episode finished after 444 timesteps\n",
      "Episode finished after 463 timesteps\n",
      "Episode finished after 485 timesteps\n",
      "Episode finished after 509 timesteps\n",
      "Episode finished after 528 timesteps\n",
      "Episode finished after 547 timesteps\n",
      "Episode finished after 559 timesteps\n",
      "Episode finished after 576 timesteps\n",
      "Episode finished after 586 timesteps\n",
      "Episode finished after 610 timesteps\n",
      "Episode finished after 653 timesteps\n",
      "Episode finished after 680 timesteps\n",
      "Episode finished after 690 timesteps\n",
      "Episode finished after 740 timesteps\n",
      "Episode finished after 772 timesteps\n",
      "Episode finished after 786 timesteps\n",
      "Episode finished after 841 timesteps\n",
      "Episode finished after 874 timesteps\n",
      "Episode finished after 883 timesteps\n",
      "Episode finished after 893 timesteps\n",
      "Episode finished after 910 timesteps\n",
      "Episode finished after 938 timesteps\n",
      "Episode finished after 969 timesteps\n",
      "Episode finished after 996 timesteps\n",
      "Episode finished after 1008 timesteps\n",
      "Episode finished after 1052 timesteps\n",
      "Episode finished after 1076 timesteps\n",
      "Episode finished after 1099 timesteps\n",
      "Episode finished after 1111 timesteps\n",
      "Episode finished after 1143 timesteps\n",
      "Episode finished after 1181 timesteps\n",
      "Episode finished after 1210 timesteps\n",
      "Episode finished after 1223 timesteps\n",
      "Episode finished after 1255 timesteps\n",
      "Episode finished after 1286 timesteps\n",
      "Episode finished after 1318 timesteps\n",
      "Episode finished after 1334 timesteps\n",
      "Episode finished after 1350 timesteps\n",
      "Episode finished after 1375 timesteps\n",
      "Episode finished after 1408 timesteps\n",
      "Episode finished after 1420 timesteps\n",
      "Episode finished after 1434 timesteps\n",
      "Episode finished after 1460 timesteps\n",
      "Episode finished after 1477 timesteps\n",
      "Episode finished after 1488 timesteps\n",
      "Episode finished after 1501 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 39 timesteps\n",
      "Episode finished after 56 timesteps\n",
      "Episode finished after 82 timesteps\n",
      "Episode finished after 93 timesteps\n",
      "Episode finished after 117 timesteps\n",
      "Episode finished after 132 timesteps\n",
      "Episode finished after 171 timesteps\n",
      "Episode finished after 198 timesteps\n",
      "Episode finished after 216 timesteps\n",
      "Episode finished after 232 timesteps\n",
      "Episode finished after 244 timesteps\n",
      "Episode finished after 259 timesteps\n",
      "Episode finished after 12 timesteps\n",
      "Episode finished after 37 timesteps\n",
      "Episode finished after 53 timesteps\n",
      "Episode finished after 88 timesteps\n",
      "Episode finished after 100 timesteps\n",
      "Episode finished after 156 timesteps\n",
      "Episode finished after 179 timesteps\n",
      "Episode finished after 192 timesteps\n",
      "Episode finished after 206 timesteps\n",
      "Episode finished after 226 timesteps\n",
      "Episode finished after 244 timesteps\n",
      "Episode finished after 264 timesteps\n",
      "Episode finished after 287 timesteps\n",
      "Episode finished after 315 timesteps\n",
      "Episode finished after 335 timesteps\n",
      "Episode finished after 355 timesteps\n",
      "Episode finished after 368 timesteps\n",
      "Episode finished after 401 timesteps\n",
      "Episode finished after 418 timesteps\n",
      "Episode finished after 433 timesteps\n",
      "Episode finished after 457 timesteps\n",
      "Episode finished after 484 timesteps\n",
      "Episode finished after 515 timesteps\n",
      "Episode finished after 546 timesteps\n",
      "Episode finished after 586 timesteps\n",
      "Episode finished after 620 timesteps\n",
      "Episode finished after 672 timesteps\n",
      "Episode finished after 690 timesteps\n",
      "Episode finished after 726 timesteps\n",
      "Episode finished after 750 timesteps\n",
      "Episode finished after 767 timesteps\n",
      "Episode finished after 775 timesteps\n",
      "Episode finished after 797 timesteps\n",
      "Episode finished after 840 timesteps\n",
      "Episode finished after 870 timesteps\n",
      "Episode finished after 883 timesteps\n",
      "Episode finished after 902 timesteps\n",
      "Episode finished after 919 timesteps\n",
      "Episode finished after 967 timesteps\n",
      "Episode finished after 979 timesteps\n",
      "Episode finished after 1008 timesteps\n",
      "Episode finished after 1033 timesteps\n",
      "Episode finished after 1052 timesteps\n",
      "Episode finished after 1064 timesteps\n",
      "Episode finished after 1076 timesteps\n",
      "Episode finished after 1098 timesteps\n",
      "Episode finished after 1120 timesteps\n",
      "Episode finished after 1143 timesteps\n",
      "Episode finished after 1162 timesteps\n",
      "Episode finished after 1176 timesteps\n",
      "Episode finished after 1199 timesteps\n",
      "Episode finished after 1211 timesteps\n",
      "Episode finished after 1231 timesteps\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "block_reward = 0.0;\n",
    "total_epochs = 0;\n",
    "\n",
    "\n",
    "#block_reward = np.zeros((16), dtype=float);\n",
    "#total_epochs = np.zeros((16), dtype=float);\n",
    "   \n",
    "# Create estimators\n",
    "q_estimator = Estimator(nfeats*nwindow, nhidden, nactions)\n",
    "target_estimator = Estimator(nfeats*nwindow, nhidden, nactions)\n",
    "\n",
    "# The epsilon and learning rate decay schedules\n",
    "epsilons = np.linspace(epsilon_start, epsilon_end, neps)\n",
    "learning_rates = np.linspace(learning_rate, lr_end, nlr)\n",
    "\n",
    "# Initialize the games\n",
    "print(\"Initializing games...\"); sys.stdout.flush()\n",
    "envs = np.empty(npar, dtype=object)\n",
    "state = np.zeros([nfeats * nwindow, npar], dtype=float)\n",
    "rewards = np.zeros([npar], dtype=float)\n",
    "dones = np.empty(npar, dtype=int)\n",
    "actions = np.zeros([npar], dtype=int)\n",
    "\n",
    "\n",
    "for i in range(npar):\n",
    "    envs[i] = gym.make(game_type)\n",
    "   \n",
    "    ##################################################################################\n",
    "    ##                                                                              ##\n",
    "    ## TODO: Advance each environment by a random number of steps, where the number ##\n",
    "    ##       of steps is sampled uniformly from [nwindow, init_moves].              ##\n",
    "    ##       Use random steps to advance.                                           ## \n",
    "    ##                                                                              ##\n",
    "    ## Update the total reward and total epochs variables as you go.                ##\n",
    "    ## If an environment returns done=True, reset it and increment the epoch count. ##\n",
    "    ##                                                                              ##\n",
    "    ##################################################################################\n",
    "    game_len = random.randint(nwindow, init_moves)\n",
    "    observation = envs[i].reset()\n",
    "    for t in range(game_len):\n",
    "        actions[i] = envs[i].action_space.sample()\n",
    "        obs_prev = observation\n",
    "        observation, rewards[i], dones[i], _ = envs[i].step(actions[i])\n",
    "        if dones[i]:\n",
    "            total_epochs += 1\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            rewards[i] = 0 \n",
    "            observation = envs[i].reset()\n",
    "        block_reward += rewards[i]\n",
    "    state[:, i] = np.hstack([observation, float(1), obs_prev, float(1)])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14264.0\n",
      "668\n",
      "21.3532934132\n"
     ]
    }
   ],
   "source": [
    "print(block_reward)\n",
    "print(total_epochs)\n",
    "print(block_reward / total_epochs) #Est. avg. Reward - right now ~20 much lower than 195..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement Deep Q-Learning\n",
    "In this cell you actually implement the algorithm. We've given you comments to define all the steps. You should also add book-keeping steps to keep track of the loss, reward and number of epochs (where env.step() returns done = true). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, time 0.1, loss 0.00136677, epochs 1, reward/epoch 16.00000\n",
      "step 1000, time 1.6, loss 0.51148979, epochs 690, reward/epoch 23.22206\n",
      "step 2000, time 3.0, loss 2.63448190, epochs 1243, reward/epoch 28.93309\n",
      "step 3000, time 4.5, loss 4.58896584, epochs 1612, reward/epoch 43.36043\n",
      "step 4000, time 5.9, loss 5.55076316, epochs 1841, reward/epoch 69.86900\n",
      "step 5000, time 7.4, loss 5.79970656, epochs 1984, reward/epoch 111.88811\n",
      "step 6000, time 8.8, loss 5.55254626, epochs 2076, reward/epoch 173.91304\n",
      "step 7000, time 10.3, loss 7.02600855, epochs 2177, reward/epoch 158.41584\n",
      "step 8000, time 11.7, loss 6.16711280, epochs 2252, reward/epoch 213.33333\n",
      "step 9000, time 13.1, loss 12.44262752, epochs 2339, reward/epoch 183.90805\n",
      "step 10000, time 14.7, loss 14.69702887, epochs 2432, reward/epoch 172.04301\n"
     ]
    }
   ],
   "source": [
    "##### ******* Ashwin helped me a lot on this one ******** #####\n",
    "t0 = time.time()\n",
    "block_loss = 0.0\n",
    "last_epochs=0\n",
    "total_epochs = 0\n",
    "block_reward = 0.0 \n",
    "\n",
    "for istep in np.arange(nsteps): \n",
    "    if (render): envs[0].render()\n",
    "  \n",
    "    #########################################################################\n",
    "    ## TODO: Implement Q-Learning                                          ##\n",
    "    ##                                                                     ##\n",
    "    ## At high level, your code should:                                    ##\n",
    "    ## * Update epsilon and learning rate.                                 ##\n",
    "    ## * Update target estimator from Q-estimator if needed.               ##\n",
    "    ## * Get the next action probabilities for the minibatch by running    ##\n",
    "    ##   the policy on the current state with the Q-estimator.             ##\n",
    "    ## * Then for each environment:                                        ##\n",
    "    ##     ** Pick an action according to the action probabilities.        ##\n",
    "    ##     ** Step in the gym with that action.                            ##\n",
    "    ##     ** Process the observation and concat it to the last nwindow-1  ##\n",
    "    ##        processed observations to form a new state.                  ##\n",
    "    ## Then for all environments (vectorized):                             ##\n",
    "    ## * Predict Q-scores for the new state using the target estimator.    ##\n",
    "    ## * Compute new expected rewards using those Q-scores.                ##\n",
    "    ## * Using those expected rewards as a target, compute gradients and   ##\n",
    "    ##   update the Q-estimator.                                           ##\n",
    "    ## * Step to the new state.                                            ##\n",
    "    ##                                                                     ##\n",
    "    #########################################################################    \n",
    "    next_state = np.zeros_like(state)\n",
    "    curr_index = int(istep * neps / nsteps) \n",
    "    curr_eps = epsilons[curr_index]\n",
    "    curr_index = int(istep * nlr / nsteps)\n",
    "    curr_lr = learning_rates[curr_index]\n",
    "    \n",
    "    update_estimator(target_estimator, q_estimator, target_window, istep)\n",
    "    next_action_probs = policy(q_estimator, state, curr_eps)\n",
    "    \n",
    "    for i in range(npar):\n",
    "        current_state = state[:,i]\n",
    "        action_prob = next_action_probs[:,i]\n",
    "        rand_action = np.random.choice(VALID_ACTIONS, p=action_prob)\n",
    "        observation, rew, done, _ = envs[i].step(rand_action)\n",
    "        rewards[i] = rew\n",
    "        actions[i] = rand_action\n",
    "        dones[i] = done\n",
    "        block_reward += rew\n",
    "        if done:\n",
    "            observation = envs[i].reset() #Resart and clear state\n",
    "            total_epochs += 1\n",
    "        else:\n",
    "            next_state[:,i] = np.concatenate((preprocess(observation),current_state[0:(nwindow-1)*nfeats]))\n",
    "         \n",
    "    pred_scores = target_estimator.predict(next_state)\n",
    "    max_q = np.max(pred_scores, axis=0) \n",
    "    expected_rewards = rewards\n",
    "    for i in range(npar):\n",
    "        if not dones[i]:\n",
    "            expected_rewards[i] += discount_factor * max_q[i]\n",
    "            \n",
    "    loss = q_estimator.gradient(state, actions, expected_rewards)\n",
    "    block_loss += loss \n",
    "    q_estimator.rmsprop(curr_lr, decay_rate)\n",
    "    state = next_state\n",
    "        \n",
    "    t = time.time() - t0\n",
    "    if (istep % printsteps == 0): \n",
    "        print(\"step {:0d}, time {:.1f}, loss {:.8f}, epochs {:0d}, reward/epoch {:.5f}\".format(\n",
    "                istep, t, block_loss/printsteps, total_epochs, block_reward/np.maximum(1,total_epochs-last_epochs)))\n",
    "        last_epochs = total_epochs\n",
    "        block_reward = 0.0\n",
    "        block_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the model now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(q_estimator.model, open(\"cartpole_q_estimator.p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can reload the model later if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_estimator = Estimator(nfeats*nwindow, nhidden, nactions)\n",
    "test_estimator.model = pickle.load(open(\"cartpole_q_estimator.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And animate the model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state0 = state[:,0]\n",
    "for i in np.arange(200):\n",
    "    envs[0].render()\n",
    "    preds = test_estimator.predict(state0)\n",
    "    iaction = np.argmax(preds)\n",
    "    obs, _, done0, _ = envs[0].step(VALID_ACTIONS[iaction])\n",
    "    state0 = np.concatenate((state0[nfeats:], preprocess(obs)))\n",
    "    if done0:\n",
    "        print(i)\n",
    "    if (done0): envs[0].reset()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there we have it. Simple 1-step Q-Learning can solve easy problems very fast. Note that environments that produce images will be much slower to train on than environments (like CartPole) which return an observation of the state of the system. But this model can still train on those image-based games - like Atari games. It will take hours-days however. It you try training on visual environments, we recommend you run the most expensive step - rmsprop - less often (e.g. every 10 iterations). This gives about a 3x speedup. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optional\n",
    "Do **one** of the following tasks:\n",
    "* Adapt the DQN algorithm to another environment - it can use direct state observations.  Call <code>env.get_action_meanings()</code> to find out what actions are allowed. Summarize training performance: your final average reward/epoch, the number of steps required to train, and any modifications to the model or its parameters that you made.\n",
    "* Try smarter schedules for epsilon and learning rate. Rewards for CartPole increase very sharply (several orders of magnitude) with better policies, especially as epsilon --> 0. Gradients will also change drastically, so the initial learning rate is probably not good later on. Try schedules for decreasing epsilon that allow the model to better adapt. Try other learning rate schedules, or setting learning rate based on average reward. \n",
    "* Try a fancier model. e.g. add another hidden layer, or try sigmoid non-linearities."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
