{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n",
    "\n",
    "In this notebook, you will implement a deep Q-Learning reinforcement algorithm. The implementation borrows ideas from both the original DeepMind Nature paper and the more recent asynchronous version:<br/>\n",
    "[1] \"Human-Level Control through Deep Reinforcement Learning\" by Mnih et al. 2015<br/>\n",
    "[2] \"Asynchronous Methods for Deep Reinforcement Learning\" by Mnih et al. 2016.<br/>\n",
    "\n",
    "In particular:\n",
    "* We use separate target and Q-functions estimators with periodic updates to the target estimator. \n",
    "* We use several concurrent \"threads\" rather than experience replay to generate less biased gradient updates. \n",
    "* Threads are actually synchronized so we start each one at a random number of moves.\n",
    "* We use an epsilon-greedy policy that blends random moves with policy moves.\n",
    "* We taper the random action parameter (epsilon) and the learning rate to zero during training.\n",
    "\n",
    "This gives a simple and reasonably fast general-purpose RL algorithm. We use it here for the Cartpole environment from OpenAI Gym, but it can easily be adapted to others. For this notebook, you will implement 4 steps:\n",
    "\n",
    "1. The backward step for the Q-estimator\n",
    "2. The $\\epsilon$-greedy policy\n",
    "3. \"asynchronous\" initialization \n",
    "4. The Q-learning algorithm\n",
    "\n",
    "To get started, we import some prerequisites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajagrawal/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below lists some parameters you can tune. They should be self-explanatory. They are currently set to train CartPole-V0 to a \"solved\" score (> 195) most of the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nsteps = 10001                       # Number of steps to run (game actions per environment)\n",
    "npar = 16                            # Number of parallel environments\n",
    "target_window = 200                  # Interval to update target estimator from q-estimator\n",
    "discount_factor = 0.99               # Reward discount factor\n",
    "printsteps = 1000                    # Number of steps between printouts\n",
    "render = False                       # Whether to render an environment while training\n",
    "\n",
    "epsilon_start = 1.0                  # Parameters for epsilon-greedy policy: initial epsilon\n",
    "epsilon_end = 0.0                    # Final epsilon\n",
    "neps = int(0.8*nsteps)               # Number of steps to decay epsilon\n",
    "\n",
    "learning_rate = 2e-3                 # Initial learning rate\n",
    "lr_end = 0                           # Final learning rate\n",
    "nlr = neps                           # Steps to decay learning rate\n",
    "decay_rate = 0.99                    # Decay factor for RMSProp \n",
    "\n",
    "nhidden = 200                        # Number of hidden layers for estimators\n",
    "\n",
    "init_moves = 2000                    # Upper bound on random number of moves to take initially\n",
    "nwindow = 2                          # Sensing window = last n images in a state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are environment-specific parameters. The function \"preprocess\" should process an observation returned by the environment into a vector for training. For CartPole we simply append a 1 to implement bias in the first layer. \n",
    "\n",
    "For visual environments you would typically crop, downsample to 80x80, set color to a single bit (foreground/background), and flatten to a vector. That transformation is already implemented in the Policy Gradient code.\n",
    "\n",
    "*nfeats* is the dimension of the vector output by *preprocess*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "game_type=\"CartPole-v0\"                 # Model type and action definitions\n",
    "VALID_ACTIONS = [0, 1]\n",
    "nactions = len(VALID_ACTIONS)\n",
    "nfeats = 5                              # There are four state features plus the constant we add\n",
    "\n",
    "def preprocess(I):                      # preprocess each observation\n",
    "    \"\"\"Just append a 1 to the end\"\"\"\n",
    "    return np.append(I.astype(float),1) # Add a constant feature for bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Q-estimator class. We use two instances of this class, one for the target estimator, and one for the Q-estimator. The Q function is normally represented as a scalar $Q(x,a)$ where $x$ is the state and $a$ is an action. For ease of implementation, we actually estimate a vector-valued function $Q(x,.)$ which returns the estimated reward for every action. The model here has just a single hidden layer:\n",
    "\n",
    "<pre>\n",
    "Input Layer (nfeats) => FC Layer => RELU => FC Layer => Output (naction values)\n",
    "</pre>\n",
    "\n",
    "## 1. Implement Q-estimator gradient\n",
    "Your first task is to implement the\n",
    "<pre>Estimator.gradient(s, a, y)</pre>\n",
    "method for this class. **gradient** should compute the gradients wrt weight arrays W1 and W2 into\n",
    "<pre>self.grad['W1']\n",
    "self.grad['W2']</pre>\n",
    "respectively. Both <code>a</code> and <code>y</code> are vectors. Be sure to update only the output layer weights corresponding to the given action vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "\n",
    "    def __init__(self, ninputs, nhidden, nactions):\n",
    "        \"\"\" Create model matrices, and gradient and squared gradient buffers\"\"\"\n",
    "        model = {}\n",
    "        model['W1'] = np.random.randn(nhidden, ninputs) / np.sqrt(ninputs)   # \"Xavier\" initialization\n",
    "        model['W2'] = np.random.randn(nactions, nhidden) / np.sqrt(nhidden)\n",
    "        self.model = model\n",
    "        self.grad = { k : np.zeros_like(v) for k,v in model.iteritems() }\n",
    "        self.gradsq = { k : np.zeros_like(v) for k,v in model.iteritems() }   \n",
    "        \n",
    "\n",
    "    def forward(self, s):\n",
    "        \"\"\" Run the model forward given a state as input.\n",
    "    returns action predictions and the hidden state\"\"\"\n",
    "        h = np.dot(self.model['W1'], s)\n",
    "        h[h<0] = 0 # ReLU nonlinearity\n",
    "        rew = np.dot(self.model['W2'], h)\n",
    "        return rew, h\n",
    "    \n",
    "    \n",
    "    def predict(self, s):\n",
    "        \"\"\" Predict the action rewards from a given input state\"\"\"\n",
    "        rew, h = self.forward(s)\n",
    "        return rew\n",
    "    \n",
    "              \n",
    "    def gradient(self, s, a, y):\n",
    "        \"\"\" Given a state s, action a and target y, compute the model gradients\"\"\"\n",
    "    ##################################################################################\n",
    "    ##                                                                              ##\n",
    "    ## TODO: Compute gradients and return a scalar loss on a minibatch of size npar ##\n",
    "    ##    s is the input state matrix (ninputs x npar).                             ##\n",
    "    ##    a is an action vector (npar,).                                            ##\n",
    "    ##    y is a vector of target values (npar,) corresponding to those actions.    ##\n",
    "    ##    return: the loss per sample (npar,).                                      ##                          \n",
    "    ##                                                                              ##\n",
    "    ## Notes:                                                                       ##\n",
    "    ##    * If the action is ai in [0,...,nactions-1], backprop only through the    ##\n",
    "    ##      ai'th output.                                                           ##\n",
    "    ##    * loss should be L2, and we recommend you normalize it to a per-input     ##\n",
    "    ##      value, i.e. return L2(target,predition)/sqrt(npar).                     ##\n",
    "    ##    * save the gradients in self.grad['W1'] and self.grad['W2'].              ##\n",
    "    ##    * update self.grad['W1'] and self.grad['W2'] by adding the gradients, so  ##\n",
    "    ##      that multiple gradient steps can be used beteween updates.              ##\n",
    "    ##                                                                              ##\n",
    "    ##################################################################################\n",
    "        loss = 0.0 \n",
    "        prediction, h = self.forward(s)\n",
    "        diffs = np.zeros(npar)\n",
    "        dW2 = np.zeros_like(self.model['W2'])\n",
    "        dW1 = np.zeros_like(self.model['W1'])\n",
    "        num_hidden = h.shape[0]\n",
    "        for i in range(npar):\n",
    "            si = s[:, i]\n",
    "            ai = a[i]\n",
    "            hi = h[:, i]\n",
    "            pred_i = prediction[ai, i]\n",
    "            residual = pred_i - y[i]\n",
    "            diffs[i] = residual \n",
    "            dW2[ai, :] += 2 * residual * hi \n",
    "            dhidden = 2 * residual * self.model['W2'][ai, :]\n",
    "            dhidden[hi == 0] = 0 \n",
    "            dW1 += np.outer(dhidden, si)\n",
    "        \n",
    "        loss = np.linalg.norm(diffs, 2)\n",
    "        constant = 1 / (2 * loss * np.sqrt(npar))\n",
    "        dW1 *= constant\n",
    "        dW2 *= constant\n",
    "        \n",
    "        self.grad['W1'] = dW1\n",
    "        self.grad['W2'] = dW2\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def rmsprop(self, learning_rate, decay_rate): \n",
    "        \"\"\" Perform model updates from the gradients using RMSprop\"\"\"\n",
    "        for k in self.model:\n",
    "            g = self.grad[k]\n",
    "            self.gradsq[k] = decay_rate * self.gradsq[k] + (1 - decay_rate) * g*g\n",
    "            self.model[k] -= learning_rate * g / (np.sqrt(self.gradsq[k]) + 1e-5)\n",
    "            self.grad[k].fill(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient Checking \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement $\\epsilon$-Greedy Policy\n",
    "\n",
    "An $\\epsilon$-Greedy policy should:\n",
    "* with probability $\\epsilon$ take a uniformly-random action.\n",
    "* otherwise choose the best action according to the estimator from the given state.\n",
    "\n",
    "The function below should implement this policy. It should return a matrix A of size (nactions, npar) such that A[i,j] is the probability of taking action i on input j. The probabilities of non-optimal actions should be $\\epsilon/{\\rm nactions}$ and the probability of the best action should be $1-\\epsilon+\\epsilon/{\\rm nactions}$.\n",
    "\n",
    "Since the function processes batches of states, the input <code>state</code> is a <code>ninputs x npar</code> matrix, and the returned value should be a <code>nactions x npar</code> matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy(estimator, state, epsilon):\n",
    "    \"\"\" Take an estimator and state and predict the best action.\n",
    "    For each input state, return a vector of action probabilities according to an epsilon-greedy policy\"\"\"\n",
    "    ##################################################################################\n",
    "    ##                                                                              ##\n",
    "    ## TODO: Implement an epsilon-greedy policy                                     ##\n",
    "    ##       estimator: is the estimator to use (instance of Estimator)             ##\n",
    "    ##       state is an (ninputs x npar) state matrix                              ##\n",
    "    ##       epsilon is the scalar policy parameter                                 ##\n",
    "    ## return: an (nactions x npar) matrix A where A[i,j] is the probability of     ##\n",
    "    ##       taking action i on input j.                                            ##\n",
    "    ##                                                                              ##\n",
    "    ## Use the definition of epsilon-greedy from the cell above.                    ##\n",
    "    ##                                                                              ##\n",
    "    ##################################################################################\n",
    "    predictions = estimator.predict(state)\n",
    "    nactions, npar = predictions.shape\n",
    "    best_action = np.argmax(predictions, axis=0)\n",
    "    A = np.zeros((nactions, npar))\n",
    "    A += epsilon/nactions\n",
    "    for i in range(npar):\n",
    "        A[best_action[i], i] = 1 - epsilon + (epsilon / nactions)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This routine copies the state of one estimator into another. Its used to update the target estimator from the Q-estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_estimator(to_estimator, from_estimator, window, istep):\n",
    "    \"\"\" every <window> steps, Copy model state from from_estimator into to_estimator\"\"\"\n",
    "    if (istep % window == 0):\n",
    "        for k in from_estimator.model:\n",
    "            np.copyto(to_estimator.model[k], from_estimator.model[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement \"Asynchronous Threads\"\n",
    "\n",
    "Don't try that in Python!! Actually all we do here is create an array of environments and advance each one a random number of steps, using random actions at each step. Later on we will make *synchronous* updates to all the environments, but the environments (and their gradient updates) should remain uncorrelated. This serves the same goal as asynchronous updates in paper [2], or experience replay in paper [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-22 01:48:05,570] Making new env: CartPole-v0\n",
      "[2016-11-22 01:48:05,611] Making new env: CartPole-v0\n",
      "[2016-11-22 01:48:05,650] Making new env: CartPole-v0\n",
      "[2016-11-22 01:48:05,663] Making new env: CartPole-v0\n",
      "[2016-11-22 01:48:05,708] Making new env: CartPole-v0\n",
      "[2016-11-22 01:48:05,740] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 30 timesteps\n",
      "Episode finished after 46 timesteps\n",
      "Episode finished after 59 timesteps\n",
      "Episode finished after 94 timesteps\n",
      "Episode finished after 113 timesteps\n",
      "Episode finished after 140 timesteps\n",
      "Episode finished after 178 timesteps\n",
      "Episode finished after 192 timesteps\n",
      "Episode finished after 220 timesteps\n",
      "Episode finished after 247 timesteps\n",
      "Episode finished after 260 timesteps\n",
      "Episode finished after 276 timesteps\n",
      "Episode finished after 290 timesteps\n",
      "Episode finished after 313 timesteps\n",
      "Episode finished after 326 timesteps\n",
      "Episode finished after 356 timesteps\n",
      "Episode finished after 375 timesteps\n",
      "Episode finished after 396 timesteps\n",
      "Episode finished after 411 timesteps\n",
      "Episode finished after 433 timesteps\n",
      "Episode finished after 457 timesteps\n",
      "Episode finished after 471 timesteps\n",
      "Episode finished after 482 timesteps\n",
      "Episode finished after 518 timesteps\n",
      "Episode finished after 536 timesteps\n",
      "Episode finished after 552 timesteps\n",
      "Episode finished after 572 timesteps\n",
      "Episode finished after 607 timesteps\n",
      "Episode finished after 633 timesteps\n",
      "Episode finished after 648 timesteps\n",
      "Episode finished after 665 timesteps\n",
      "Episode finished after 676 timesteps\n",
      "Episode finished after 697 timesteps\n",
      "Episode finished after 724 timesteps\n",
      "Episode finished after 751 timesteps\n",
      "Episode finished after 762 timesteps\n",
      "Episode finished after 773 timesteps\n",
      "Episode finished after 788 timesteps\n",
      "Episode finished after 824 timesteps\n",
      "Episode finished after 846 timesteps\n",
      "Episode finished after 862 timesteps\n",
      "Episode finished after 886 timesteps\n",
      "Episode finished after 899 timesteps\n",
      "Episode finished after 918 timesteps\n",
      "Episode finished after 928 timesteps\n",
      "Episode finished after 976 timesteps\n",
      "Episode finished after 998 timesteps\n",
      "Episode finished after 1009 timesteps\n",
      "Episode finished after 1032 timesteps\n",
      "Episode finished after 1059 timesteps\n",
      "Episode finished after 1070 timesteps\n",
      "Episode finished after 1101 timesteps\n",
      "Episode finished after 1117 timesteps\n",
      "Episode finished after 1180 timesteps\n",
      "Episode finished after 1196 timesteps\n",
      "Episode finished after 1212 timesteps\n",
      "Episode finished after 1279 timesteps\n",
      "Episode finished after 1300 timesteps\n",
      "Episode finished after 1322 timesteps\n",
      "Episode finished after 1337 timesteps\n",
      "Episode finished after 1364 timesteps\n",
      "Episode finished after 1376 timesteps\n",
      "Episode finished after 1390 timesteps\n",
      "Episode finished after 1414 timesteps\n",
      "Episode finished after 22 timesteps\n",
      "Episode finished after 77 timesteps\n",
      "Episode finished after 99 timesteps\n",
      "Episode finished after 117 timesteps\n",
      "Episode finished after 132 timesteps\n",
      "Episode finished after 159 timesteps\n",
      "Episode finished after 172 timesteps\n",
      "Episode finished after 217 timesteps\n",
      "Episode finished after 234 timesteps\n",
      "Episode finished after 246 timesteps\n",
      "Episode finished after 263 timesteps\n",
      "Episode finished after 279 timesteps\n",
      "Episode finished after 321 timesteps\n",
      "Episode finished after 341 timesteps\n",
      "Episode finished after 364 timesteps\n",
      "Episode finished after 376 timesteps\n",
      "Episode finished after 388 timesteps\n",
      "Episode finished after 418 timesteps\n",
      "Episode finished after 453 timesteps\n",
      "Episode finished after 467 timesteps\n",
      "Episode finished after 485 timesteps\n",
      "Episode finished after 502 timesteps\n",
      "Episode finished after 515 timesteps\n",
      "Episode finished after 558 timesteps\n",
      "Episode finished after 577 timesteps\n",
      "Episode finished after 592 timesteps\n",
      "Episode finished after 603 timesteps\n",
      "Episode finished after 623 timesteps\n",
      "Episode finished after 648 timesteps\n",
      "Episode finished after 670 timesteps\n",
      "Episode finished after 710 timesteps\n",
      "Episode finished after 730 timesteps\n",
      "Episode finished after 750 timesteps\n",
      "Episode finished after 778 timesteps\n",
      "Episode finished after 800 timesteps\n",
      "Episode finished after 810 timesteps\n",
      "Episode finished after 851 timesteps\n",
      "Episode finished after 868 timesteps\n",
      "Episode finished after 896 timesteps\n",
      "Episode finished after 917 timesteps\n",
      "Episode finished after 951 timesteps\n",
      "Episode finished after 960 timesteps\n",
      "Episode finished after 982 timesteps\n",
      "Episode finished after 992 timesteps\n",
      "Episode finished after 1001 timesteps\n",
      "Episode finished after 1025 timesteps\n",
      "Episode finished after 1043 timesteps\n",
      "Episode finished after 1073 timesteps\n",
      "Episode finished after 1094 timesteps\n",
      "Episode finished after 1109 timesteps\n",
      "Episode finished after 1120 timesteps\n",
      "Episode finished after 1136 timesteps\n",
      "Episode finished after 1149 timesteps\n",
      "Episode finished after 1183 timesteps\n",
      "Episode finished after 1207 timesteps\n",
      "Episode finished after 1222 timesteps\n",
      "Episode finished after 1231 timesteps\n",
      "Episode finished after 1273 timesteps\n",
      "Episode finished after 1293 timesteps\n",
      "Episode finished after 1309 timesteps\n",
      "Episode finished after 1329 timesteps\n",
      "Episode finished after 1345 timesteps\n",
      "Episode finished after 1359 timesteps\n",
      "Episode finished after 1374 timesteps\n",
      "Episode finished after 1394 timesteps\n",
      "Episode finished after 1412 timesteps\n",
      "Episode finished after 1426 timesteps\n",
      "Episode finished after 1447 timesteps\n",
      "Episode finished after 1493 timesteps\n",
      "Episode finished after 1512 timesteps\n",
      "Episode finished after 1526 timesteps\n",
      "Episode finished after 1539 timesteps\n",
      "Episode finished after 1556 timesteps\n",
      "Episode finished after 1574 timesteps\n",
      "Episode finished after 1585 timesteps\n",
      "Episode finished after 1603 timesteps\n",
      "Episode finished after 1615 timesteps\n",
      "Episode finished after 1636 timesteps\n",
      "Episode finished after 1669 timesteps\n",
      "Episode finished after 1678 timesteps\n",
      "Episode finished after 1701 timesteps\n",
      "Episode finished after 1721 timesteps\n",
      "Episode finished after 1734 timesteps\n",
      "Episode finished after 1744 timesteps\n",
      "Episode finished after 1756 timesteps\n",
      "Episode finished after 30 timesteps\n",
      "Episode finished after 57 timesteps\n",
      "Episode finished after 84 timesteps\n",
      "Episode finished after 96 timesteps\n",
      "Episode finished after 120 timesteps\n",
      "Episode finished after 139 timesteps\n",
      "Episode finished after 155 timesteps\n",
      "Episode finished after 170 timesteps\n",
      "Episode finished after 43 timesteps\n",
      "Episode finished after 66 timesteps\n",
      "Episode finished after 84 timesteps\n",
      "Episode finished after 110 timesteps\n",
      "Episode finished after 123 timesteps\n",
      "Episode finished after 150 timesteps\n",
      "Episode finished after 189 timesteps\n",
      "Episode finished after 207 timesteps\n",
      "Episode finished after 217 timesteps\n",
      "Episode finished after 234 timesteps\n",
      "Episode finished after 262 timesteps\n",
      "Episode finished after 270 timesteps\n",
      "Episode finished after 296 timesteps\n",
      "Episode finished after 311 timesteps\n",
      "Episode finished after 333 timesteps\n",
      "Episode finished after 351 timesteps\n",
      "Episode finished after 396 timesteps\n",
      "Episode finished after 441 timesteps\n",
      "Episode finished after 459 timesteps\n",
      "Episode finished after 477 timesteps\n",
      "Episode finished after 496 timesteps\n",
      "Episode finished after 513 timesteps\n",
      "Episode finished after 531 timesteps\n",
      "Episode finished after 545 timesteps\n",
      "Episode finished after 568 timesteps\n",
      "Episode finished after 589 timesteps\n",
      "Episode finished after 605 timesteps\n",
      "Episode finished after 620 timesteps\n",
      "Episode finished after 634 timesteps\n",
      "Episode finished after 649 timesteps\n",
      "Episode finished after 664 timesteps\n",
      "Episode finished after 678 timesteps\n",
      "Episode finished after 690 timesteps\n",
      "Episode finished after 702 timesteps\n",
      "Episode finished after 716 timesteps\n",
      "Episode finished after 729 timesteps\n",
      "Episode finished after 740 timesteps\n",
      "Episode finished after 750 timesteps\n",
      "Episode finished after 766 timesteps\n",
      "Episode finished after 790 timesteps\n",
      "Episode finished after 803 timesteps\n",
      "Episode finished after 830 timesteps\n",
      "Episode finished after 843 timesteps\n",
      "Episode finished after 860 timesteps\n",
      "Episode finished after 874 timesteps\n",
      "Episode finished after 894 timesteps\n",
      "Episode finished after 905 timesteps\n",
      "Episode finished after 924 timesteps\n",
      "Episode finished after 936 timesteps\n",
      "Episode finished after 966 timesteps\n",
      "Episode finished after 998 timesteps\n",
      "Episode finished after 1016 timesteps\n",
      "Episode finished after 1029 timesteps\n",
      "Episode finished after 1040 timesteps\n",
      "Episode finished after 1070 timesteps\n",
      "Episode finished after 1100 timesteps\n",
      "Episode finished after 1121 timesteps\n",
      "Episode finished after 1149 timesteps\n",
      "Episode finished after 1164 timesteps\n",
      "Episode finished after 1193 timesteps\n",
      "Episode finished after 1211 timesteps\n",
      "Episode finished after 1222 timesteps\n",
      "Episode finished after 1238 timesteps\n",
      "Episode finished after 1252 timesteps\n",
      "Episode finished after 1271 timesteps\n",
      "Episode finished after 1285 timesteps\n",
      "Episode finished after 1301 timesteps\n",
      "Episode finished after 1321 timesteps\n",
      "Episode finished after 1341 timesteps\n",
      "Episode finished after 1368 timesteps\n",
      "Episode finished after 1385 timesteps\n",
      "Episode finished after 12 timesteps\n",
      "Episode finished after 49 timesteps\n",
      "Episode finished after 67 timesteps\n",
      "Episode finished after 78 timesteps\n",
      "Episode finished after 97 timesteps\n",
      "Episode finished after 113 timesteps\n",
      "Episode finished after 139 timesteps\n",
      "Episode finished after 158 timesteps\n",
      "Episode finished after 185 timesteps\n",
      "Episode finished after 206 timesteps\n",
      "Episode finished after 227 timesteps\n",
      "Episode finished after 239 timesteps\n",
      "Episode finished after 297 timesteps\n",
      "Episode finished after 317 timesteps\n",
      "Episode finished after 335 timesteps\n",
      "Episode finished after 350 timesteps\n",
      "Episode finished after 362 timesteps\n",
      "Episode finished after 401 timesteps\n",
      "Episode finished after 433 timesteps\n",
      "Episode finished after 462 timesteps\n",
      "Episode finished after 478 timesteps\n",
      "Episode finished after 498 timesteps\n",
      "Episode finished after 528 timesteps\n",
      "Episode finished after 544 timesteps\n",
      "Episode finished after 562 timesteps\n",
      "Episode finished after 25 timesteps\n",
      "Episode finished after 43 timesteps\n",
      "Episode finished after 91 timesteps\n",
      "Episode finished after 118 timesteps\n",
      "Episode finished after 132 timesteps\n",
      "Episode finished after 141 timesteps\n",
      "Episode finished after 182 timesteps\n",
      "Episode finished after 203 timesteps\n",
      "Episode finished after 217 timesteps\n",
      "Episode finished after 237 timesteps\n",
      "Episode finished after 249 timesteps\n",
      "Episode finished after 267 timesteps\n",
      "Episode finished after 277 timesteps\n",
      "Episode finished after 327 timesteps\n",
      "Episode finished after 342 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-22 01:48:05,787] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 353 timesteps\n",
      "Episode finished after 370 timesteps\n",
      "Episode finished after 399 timesteps\n",
      "Episode finished after 471 timesteps\n",
      "Episode finished after 486 timesteps\n",
      "Episode finished after 516 timesteps\n",
      "Episode finished after 552 timesteps\n",
      "Episode finished after 577 timesteps\n",
      "Episode finished after 589 timesteps\n",
      "Episode finished after 627 timesteps\n",
      "Episode finished after 638 timesteps\n",
      "Episode finished after 657 timesteps\n",
      "Episode finished after 668 timesteps\n",
      "Episode finished after 682 timesteps\n",
      "Episode finished after 707 timesteps\n",
      "Episode finished after 724 timesteps\n",
      "Episode finished after 737 timesteps\n",
      "Episode finished after 755 timesteps\n",
      "Episode finished after 774 timesteps\n",
      "Episode finished after 793 timesteps\n",
      "Episode finished after 807 timesteps\n",
      "Episode finished after 818 timesteps\n",
      "Episode finished after 842 timesteps\n",
      "Episode finished after 859 timesteps\n",
      "Episode finished after 871 timesteps\n",
      "Episode finished after 884 timesteps\n",
      "Episode finished after 907 timesteps\n",
      "Episode finished after 921 timesteps\n",
      "Episode finished after 939 timesteps\n",
      "Episode finished after 953 timesteps\n",
      "Episode finished after 975 timesteps\n",
      "Episode finished after 987 timesteps\n",
      "Episode finished after 1008 timesteps\n",
      "Episode finished after 29 timesteps\n",
      "Episode finished after 39 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-22 01:48:05,854] Making new env: CartPole-v0\n",
      "[2016-11-22 01:48:05,913] Making new env: CartPole-v0\n",
      "[2016-11-22 01:48:05,929] Making new env: CartPole-v0\n",
      "[2016-11-22 01:48:05,956] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 55 timesteps\n",
      "Episode finished after 76 timesteps\n",
      "Episode finished after 97 timesteps\n",
      "Episode finished after 129 timesteps\n",
      "Episode finished after 152 timesteps\n",
      "Episode finished after 170 timesteps\n",
      "Episode finished after 201 timesteps\n",
      "Episode finished after 225 timesteps\n",
      "Episode finished after 249 timesteps\n",
      "Episode finished after 268 timesteps\n",
      "Episode finished after 298 timesteps\n",
      "Episode finished after 323 timesteps\n",
      "Episode finished after 348 timesteps\n",
      "Episode finished after 365 timesteps\n",
      "Episode finished after 389 timesteps\n",
      "Episode finished after 399 timesteps\n",
      "Episode finished after 419 timesteps\n",
      "Episode finished after 428 timesteps\n",
      "Episode finished after 447 timesteps\n",
      "Episode finished after 469 timesteps\n",
      "Episode finished after 488 timesteps\n",
      "Episode finished after 503 timesteps\n",
      "Episode finished after 544 timesteps\n",
      "Episode finished after 580 timesteps\n",
      "Episode finished after 593 timesteps\n",
      "Episode finished after 607 timesteps\n",
      "Episode finished after 631 timesteps\n",
      "Episode finished after 648 timesteps\n",
      "Episode finished after 673 timesteps\n",
      "Episode finished after 687 timesteps\n",
      "Episode finished after 698 timesteps\n",
      "Episode finished after 721 timesteps\n",
      "Episode finished after 768 timesteps\n",
      "Episode finished after 786 timesteps\n",
      "Episode finished after 803 timesteps\n",
      "Episode finished after 825 timesteps\n",
      "Episode finished after 844 timesteps\n",
      "Episode finished after 860 timesteps\n",
      "Episode finished after 872 timesteps\n",
      "Episode finished after 887 timesteps\n",
      "Episode finished after 902 timesteps\n",
      "Episode finished after 919 timesteps\n",
      "Episode finished after 951 timesteps\n",
      "Episode finished after 963 timesteps\n",
      "Episode finished after 991 timesteps\n",
      "Episode finished after 1010 timesteps\n",
      "Episode finished after 1029 timesteps\n",
      "Episode finished after 1046 timesteps\n",
      "Episode finished after 1070 timesteps\n",
      "Episode finished after 1086 timesteps\n",
      "Episode finished after 1105 timesteps\n",
      "Episode finished after 1195 timesteps\n",
      "Episode finished after 1214 timesteps\n",
      "Episode finished after 1228 timesteps\n",
      "Episode finished after 1241 timesteps\n",
      "Episode finished after 1257 timesteps\n",
      "Episode finished after 1272 timesteps\n",
      "Episode finished after 1292 timesteps\n",
      "Episode finished after 1318 timesteps\n",
      "Episode finished after 1336 timesteps\n",
      "Episode finished after 1350 timesteps\n",
      "Episode finished after 1368 timesteps\n",
      "Episode finished after 1394 timesteps\n",
      "Episode finished after 1441 timesteps\n",
      "Episode finished after 1460 timesteps\n",
      "Episode finished after 1474 timesteps\n",
      "Episode finished after 1488 timesteps\n",
      "Episode finished after 1521 timesteps\n",
      "Episode finished after 1538 timesteps\n",
      "Episode finished after 1571 timesteps\n",
      "Episode finished after 1583 timesteps\n",
      "Episode finished after 1594 timesteps\n",
      "Episode finished after 1613 timesteps\n",
      "Episode finished after 1626 timesteps\n",
      "Episode finished after 1673 timesteps\n",
      "Episode finished after 1706 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 26 timesteps\n",
      "Episode finished after 42 timesteps\n",
      "Episode finished after 58 timesteps\n",
      "Episode finished after 102 timesteps\n",
      "Episode finished after 119 timesteps\n",
      "Episode finished after 148 timesteps\n",
      "Episode finished after 162 timesteps\n",
      "Episode finished after 189 timesteps\n",
      "Episode finished after 208 timesteps\n",
      "Episode finished after 217 timesteps\n",
      "Episode finished after 231 timesteps\n",
      "Episode finished after 251 timesteps\n",
      "Episode finished after 272 timesteps\n",
      "Episode finished after 285 timesteps\n",
      "Episode finished after 301 timesteps\n",
      "Episode finished after 318 timesteps\n",
      "Episode finished after 344 timesteps\n",
      "Episode finished after 357 timesteps\n",
      "Episode finished after 383 timesteps\n",
      "Episode finished after 405 timesteps\n",
      "Episode finished after 423 timesteps\n",
      "Episode finished after 433 timesteps\n",
      "Episode finished after 483 timesteps\n",
      "Episode finished after 502 timesteps\n",
      "Episode finished after 528 timesteps\n",
      "Episode finished after 540 timesteps\n",
      "Episode finished after 621 timesteps\n",
      "Episode finished after 647 timesteps\n",
      "Episode finished after 664 timesteps\n",
      "Episode finished after 691 timesteps\n",
      "Episode finished after 718 timesteps\n",
      "Episode finished after 730 timesteps\n",
      "Episode finished after 749 timesteps\n",
      "Episode finished after 841 timesteps\n",
      "Episode finished after 856 timesteps\n",
      "Episode finished after 881 timesteps\n",
      "Episode finished after 896 timesteps\n",
      "Episode finished after 937 timesteps\n",
      "Episode finished after 968 timesteps\n",
      "Episode finished after 977 timesteps\n",
      "Episode finished after 987 timesteps\n",
      "Episode finished after 1010 timesteps\n",
      "Episode finished after 1027 timesteps\n",
      "Episode finished after 1053 timesteps\n",
      "Episode finished after 1067 timesteps\n",
      "Episode finished after 1083 timesteps\n",
      "Episode finished after 1130 timesteps\n",
      "Episode finished after 1146 timesteps\n",
      "Episode finished after 1173 timesteps\n",
      "Episode finished after 1205 timesteps\n",
      "Episode finished after 1273 timesteps\n",
      "Episode finished after 1299 timesteps\n",
      "Episode finished after 1308 timesteps\n",
      "Episode finished after 1339 timesteps\n",
      "Episode finished after 1350 timesteps\n",
      "Episode finished after 1366 timesteps\n",
      "Episode finished after 1387 timesteps\n",
      "Episode finished after 1403 timesteps\n",
      "Episode finished after 1419 timesteps\n",
      "Episode finished after 1434 timesteps\n",
      "Episode finished after 1466 timesteps\n",
      "Episode finished after 1518 timesteps\n",
      "Episode finished after 1543 timesteps\n",
      "Episode finished after 1563 timesteps\n",
      "Episode finished after 1573 timesteps\n",
      "Episode finished after 1590 timesteps\n",
      "Episode finished after 1605 timesteps\n",
      "Episode finished after 1634 timesteps\n",
      "Episode finished after 1701 timesteps\n",
      "Episode finished after 1755 timesteps\n",
      "Episode finished after 1775 timesteps\n",
      "Episode finished after 36 timesteps\n",
      "Episode finished after 73 timesteps\n",
      "Episode finished after 90 timesteps\n",
      "Episode finished after 111 timesteps\n",
      "Episode finished after 145 timesteps\n",
      "Episode finished after 190 timesteps\n",
      "Episode finished after 207 timesteps\n",
      "Episode finished after 235 timesteps\n",
      "Episode finished after 47 timesteps\n",
      "Episode finished after 94 timesteps\n",
      "Episode finished after 111 timesteps\n",
      "Episode finished after 126 timesteps\n",
      "Episode finished after 146 timesteps\n",
      "Episode finished after 166 timesteps\n",
      "Episode finished after 203 timesteps\n",
      "Episode finished after 258 timesteps\n",
      "Episode finished after 294 timesteps\n",
      "Episode finished after 313 timesteps\n",
      "Episode finished after 330 timesteps\n",
      "Episode finished after 345 timesteps\n",
      "Episode finished after 359 timesteps\n",
      "Episode finished after 375 timesteps\n",
      "Episode finished after 396 timesteps\n",
      "Episode finished after 420 timesteps\n",
      "Episode finished after 441 timesteps\n",
      "Episode finished after 477 timesteps\n",
      "Episode finished after 489 timesteps\n",
      "Episode finished after 506 timesteps\n",
      "Episode finished after 21 timesteps\n",
      "Episode finished after 35 timesteps\n",
      "Episode finished after 49 timesteps\n",
      "Episode finished after 86 timesteps\n",
      "Episode finished after 98 timesteps\n",
      "Episode finished after 116 timesteps\n",
      "Episode finished after 130 timesteps\n",
      "Episode finished after 143 timesteps\n",
      "Episode finished after 157 timesteps\n",
      "Episode finished after 174 timesteps\n",
      "Episode finished after 195 timesteps\n",
      "Episode finished after 211 timesteps\n",
      "Episode finished after 235 timesteps\n",
      "Episode finished after 253 timesteps\n",
      "Episode finished after 269 timesteps\n",
      "Episode finished after 301 timesteps\n",
      "Episode finished after 316 timesteps\n",
      "Episode finished after 333 timesteps\n",
      "Episode finished after 349 timesteps\n",
      "Episode finished after 427 timesteps\n",
      "Episode finished after 451 timesteps\n",
      "Episode finished after 503 timesteps\n",
      "Episode finished after 524 timesteps\n",
      "Episode finished after 551 timesteps\n",
      "Episode finished after 562 timesteps\n",
      "Episode finished after 574 timesteps\n",
      "Episode finished after 592 timesteps\n",
      "Episode finished after 632 timesteps\n",
      "Episode finished after 645 timesteps\n",
      "Episode finished after 674 timesteps\n",
      "Episode finished after 689 timesteps\n",
      "Episode finished after 701 timesteps\n",
      "Episode finished after 714 timesteps\n",
      "Episode finished after 738 timesteps\n",
      "Episode finished after 752 timesteps\n",
      "Episode finished after 765 timesteps\n",
      "Episode finished after 810 timesteps\n",
      "Episode finished after 829 timesteps\n",
      "Episode finished after 844 timesteps\n",
      "Episode finished after 853 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-22 01:48:06,054] Making new env: CartPole-v0\n",
      "[2016-11-22 01:48:06,086] Making new env: CartPole-v0\n",
      "[2016-11-22 01:48:06,134] Making new env: CartPole-v0\n",
      "[2016-11-22 01:48:06,155] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 869 timesteps\n",
      "Episode finished after 903 timesteps\n",
      "Episode finished after 922 timesteps\n",
      "Episode finished after 937 timesteps\n",
      "Episode finished after 959 timesteps\n",
      "Episode finished after 975 timesteps\n",
      "Episode finished after 1008 timesteps\n",
      "Episode finished after 1019 timesteps\n",
      "Episode finished after 1048 timesteps\n",
      "Episode finished after 1110 timesteps\n",
      "Episode finished after 1120 timesteps\n",
      "Episode finished after 1157 timesteps\n",
      "Episode finished after 1172 timesteps\n",
      "Episode finished after 1201 timesteps\n",
      "Episode finished after 1230 timesteps\n",
      "Episode finished after 1239 timesteps\n",
      "Episode finished after 1256 timesteps\n",
      "Episode finished after 1277 timesteps\n",
      "Episode finished after 1297 timesteps\n",
      "Episode finished after 1310 timesteps\n",
      "Episode finished after 1320 timesteps\n",
      "Episode finished after 1347 timesteps\n",
      "Episode finished after 1387 timesteps\n",
      "Episode finished after 1415 timesteps\n",
      "Episode finished after 1437 timesteps\n",
      "Episode finished after 1456 timesteps\n",
      "Episode finished after 1482 timesteps\n",
      "Episode finished after 1514 timesteps\n",
      "Episode finished after 1548 timesteps\n",
      "Episode finished after 1563 timesteps\n",
      "Episode finished after 1577 timesteps\n",
      "Episode finished after 1587 timesteps\n",
      "Episode finished after 1601 timesteps\n",
      "Episode finished after 1627 timesteps\n",
      "Episode finished after 1654 timesteps\n",
      "Episode finished after 1668 timesteps\n",
      "Episode finished after 1684 timesteps\n",
      "Episode finished after 1699 timesteps\n",
      "Episode finished after 1721 timesteps\n",
      "Episode finished after 1750 timesteps\n",
      "Episode finished after 1780 timesteps\n",
      "Episode finished after 1803 timesteps\n",
      "Episode finished after 31 timesteps\n",
      "Episode finished after 51 timesteps\n",
      "Episode finished after 84 timesteps\n",
      "Episode finished after 104 timesteps\n",
      "Episode finished after 121 timesteps\n",
      "Episode finished after 133 timesteps\n",
      "Episode finished after 154 timesteps\n",
      "Episode finished after 170 timesteps\n",
      "Episode finished after 182 timesteps\n",
      "Episode finished after 194 timesteps\n",
      "Episode finished after 209 timesteps\n",
      "Episode finished after 243 timesteps\n",
      "Episode finished after 254 timesteps\n",
      "Episode finished after 273 timesteps\n",
      "Episode finished after 289 timesteps\n",
      "Episode finished after 309 timesteps\n",
      "Episode finished after 322 timesteps\n",
      "Episode finished after 339 timesteps\n",
      "Episode finished after 355 timesteps\n",
      "Episode finished after 372 timesteps\n",
      "Episode finished after 394 timesteps\n",
      "Episode finished after 418 timesteps\n",
      "Episode finished after 455 timesteps\n",
      "Episode finished after 482 timesteps\n",
      "Episode finished after 492 timesteps\n",
      "Episode finished after 517 timesteps\n",
      "Episode finished after 531 timesteps\n",
      "Episode finished after 550 timesteps\n",
      "Episode finished after 599 timesteps\n",
      "Episode finished after 613 timesteps\n",
      "Episode finished after 631 timesteps\n",
      "Episode finished after 646 timesteps\n",
      "Episode finished after 663 timesteps\n",
      "Episode finished after 679 timesteps\n",
      "Episode finished after 13 timesteps\n",
      "Episode finished after 26 timesteps\n",
      "Episode finished after 44 timesteps\n",
      "Episode finished after 61 timesteps\n",
      "Episode finished after 98 timesteps\n",
      "Episode finished after 130 timesteps\n",
      "Episode finished after 180 timesteps\n",
      "Episode finished after 201 timesteps\n",
      "Episode finished after 213 timesteps\n",
      "Episode finished after 240 timesteps\n",
      "Episode finished after 251 timesteps\n",
      "Episode finished after 273 timesteps\n",
      "Episode finished after 309 timesteps\n",
      "Episode finished after 320 timesteps\n",
      "Episode finished after 341 timesteps\n",
      "Episode finished after 360 timesteps\n",
      "Episode finished after 377 timesteps\n",
      "Episode finished after 399 timesteps\n",
      "Episode finished after 420 timesteps\n",
      "Episode finished after 437 timesteps\n",
      "Episode finished after 450 timesteps\n",
      "Episode finished after 459 timesteps\n",
      "Episode finished after 507 timesteps\n",
      "Episode finished after 522 timesteps\n",
      "Episode finished after 537 timesteps\n",
      "Episode finished after 546 timesteps\n",
      "Episode finished after 567 timesteps\n",
      "Episode finished after 578 timesteps\n",
      "Episode finished after 591 timesteps\n",
      "Episode finished after 604 timesteps\n",
      "Episode finished after 620 timesteps\n",
      "Episode finished after 643 timesteps\n",
      "Episode finished after 672 timesteps\n",
      "Episode finished after 691 timesteps\n",
      "Episode finished after 713 timesteps\n",
      "Episode finished after 726 timesteps\n",
      "Episode finished after 736 timesteps\n",
      "Episode finished after 751 timesteps\n",
      "Episode finished after 773 timesteps\n",
      "Episode finished after 803 timesteps\n",
      "Episode finished after 814 timesteps\n",
      "Episode finished after 841 timesteps\n",
      "Episode finished after 863 timesteps\n",
      "Episode finished after 881 timesteps\n",
      "Episode finished after 891 timesteps\n",
      "Episode finished after 913 timesteps\n",
      "Episode finished after 930 timesteps\n",
      "Episode finished after 949 timesteps\n",
      "Episode finished after 972 timesteps\n",
      "Episode finished after 994 timesteps\n",
      "Episode finished after 1009 timesteps\n",
      "Episode finished after 1023 timesteps\n",
      "Episode finished after 1048 timesteps\n",
      "Episode finished after 1097 timesteps\n",
      "Episode finished after 1113 timesteps\n",
      "Episode finished after 1124 timesteps\n",
      "Episode finished after 1196 timesteps\n",
      "Episode finished after 1210 timesteps\n",
      "Episode finished after 1220 timesteps\n",
      "Episode finished after 1264 timesteps\n",
      "Episode finished after 1284 timesteps\n",
      "Episode finished after 1299 timesteps\n",
      "Episode finished after 1315 timesteps\n",
      "Episode finished after 1331 timesteps\n",
      "Episode finished after 1357 timesteps\n",
      "Episode finished after 1370 timesteps\n",
      "Episode finished after 1391 timesteps\n",
      "Episode finished after 1404 timesteps\n",
      "Episode finished after 1463 timesteps\n",
      "Episode finished after 1499 timesteps\n",
      "Episode finished after 1521 timesteps\n",
      "Episode finished after 1541 timesteps\n",
      "Episode finished after 1573 timesteps\n",
      "Episode finished after 1597 timesteps\n",
      "Episode finished after 1611 timesteps\n",
      "Episode finished after 1645 timesteps\n",
      "Episode finished after 17 timesteps\n",
      "Episode finished after 43 timesteps\n",
      "Episode finished after 70 timesteps\n",
      "Episode finished after 107 timesteps\n",
      "Episode finished after 128 timesteps\n",
      "Episode finished after 145 timesteps\n",
      "Episode finished after 164 timesteps\n",
      "Episode finished after 174 timesteps\n",
      "Episode finished after 183 timesteps\n",
      "Episode finished after 213 timesteps\n",
      "Episode finished after 244 timesteps\n",
      "Episode finished after 270 timesteps\n",
      "Episode finished after 293 timesteps\n",
      "Episode finished after 335 timesteps\n",
      "Episode finished after 346 timesteps\n",
      "Episode finished after 359 timesteps\n",
      "Episode finished after 385 timesteps\n",
      "Episode finished after 403 timesteps\n",
      "Episode finished after 419 timesteps\n",
      "Episode finished after 444 timesteps\n",
      "Episode finished after 453 timesteps\n",
      "Episode finished after 486 timesteps\n",
      "Episode finished after 507 timesteps\n",
      "Episode finished after 575 timesteps\n",
      "Episode finished after 585 timesteps\n",
      "Episode finished after 16 timesteps\n",
      "Episode finished after 46 timesteps\n",
      "Episode finished after 62 timesteps\n",
      "Episode finished after 72 timesteps\n",
      "Episode finished after 84 timesteps\n",
      "Episode finished after 108 timesteps\n",
      "Episode finished after 125 timesteps\n",
      "Episode finished after 144 timesteps\n",
      "Episode finished after 162 timesteps\n",
      "Episode finished after 194 timesteps\n",
      "Episode finished after 208 timesteps\n",
      "Episode finished after 247 timesteps\n",
      "Episode finished after 274 timesteps\n",
      "Episode finished after 292 timesteps\n",
      "Episode finished after 305 timesteps\n",
      "Episode finished after 329 timesteps\n",
      "Episode finished after 341 timesteps\n",
      "Episode finished after 354 timesteps\n",
      "Episode finished after 369 timesteps\n",
      "Episode finished after 387 timesteps\n",
      "Episode finished after 417 timesteps\n",
      "Episode finished after 445 timesteps\n",
      "Episode finished after 475 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-22 01:48:06,192] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps\n",
      "Episode finished after 525 timesteps\n",
      "Episode finished after 558 timesteps\n",
      "Episode finished after 602 timesteps\n",
      "Episode finished after 643 timesteps\n",
      "Episode finished after 682 timesteps\n",
      "Episode finished after 693 timesteps\n",
      "Episode finished after 706 timesteps\n",
      "Episode finished after 718 timesteps\n",
      "Episode finished after 738 timesteps\n",
      "Episode finished after 751 timesteps\n",
      "Episode finished after 784 timesteps\n",
      "Episode finished after 795 timesteps\n",
      "Episode finished after 804 timesteps\n",
      "Episode finished after 831 timesteps\n",
      "Episode finished after 853 timesteps\n",
      "Episode finished after 880 timesteps\n",
      "Episode finished after 892 timesteps\n",
      "Episode finished after 909 timesteps\n",
      "Episode finished after 927 timesteps\n",
      "Episode finished after 944 timesteps\n",
      "Episode finished after 973 timesteps\n",
      "Episode finished after 992 timesteps\n",
      "Episode finished after 1019 timesteps\n",
      "Episode finished after 1056 timesteps\n",
      "Episode finished after 1096 timesteps\n",
      "Episode finished after 1112 timesteps\n",
      "Episode finished after 1135 timesteps\n",
      "Episode finished after 1157 timesteps\n",
      "Episode finished after 1178 timesteps\n",
      "Episode finished after 1198 timesteps\n",
      "Episode finished after 1236 timesteps\n",
      "Episode finished after 1255 timesteps\n",
      "Episode finished after 1268 timesteps\n",
      "Episode finished after 1304 timesteps\n",
      "Episode finished after 1327 timesteps\n",
      "Episode finished after 1345 timesteps\n",
      "Episode finished after 1358 timesteps\n",
      "Episode finished after 1378 timesteps\n",
      "Episode finished after 1403 timesteps\n",
      "Episode finished after 1414 timesteps\n",
      "Episode finished after 1439 timesteps\n",
      "Episode finished after 1456 timesteps\n",
      "Episode finished after 1483 timesteps\n",
      "Episode finished after 58 timesteps\n",
      "Episode finished after 78 timesteps\n",
      "Episode finished after 112 timesteps\n",
      "Episode finished after 135 timesteps\n",
      "Episode finished after 151 timesteps\n",
      "Episode finished after 164 timesteps\n",
      "Episode finished after 177 timesteps\n",
      "Episode finished after 208 timesteps\n",
      "Episode finished after 241 timesteps\n",
      "Episode finished after 302 timesteps\n",
      "Episode finished after 321 timesteps\n",
      "Episode finished after 335 timesteps\n",
      "Episode finished after 346 timesteps\n",
      "Episode finished after 381 timesteps\n",
      "Episode finished after 402 timesteps\n",
      "Episode finished after 412 timesteps\n",
      "Episode finished after 422 timesteps\n",
      "Episode finished after 451 timesteps\n",
      "Episode finished after 466 timesteps\n",
      "Episode finished after 478 timesteps\n",
      "Episode finished after 488 timesteps\n",
      "Episode finished after 503 timesteps\n",
      "Episode finished after 527 timesteps\n",
      "Episode finished after 550 timesteps\n",
      "Episode finished after 625 timesteps\n",
      "Episode finished after 671 timesteps\n",
      "Episode finished after 710 timesteps\n",
      "Episode finished after 744 timesteps\n",
      "Episode finished after 759 timesteps\n",
      "Episode finished after 782 timesteps\n",
      "Episode finished after 799 timesteps\n",
      "Episode finished after 813 timesteps\n",
      "Episode finished after 843 timesteps\n",
      "Episode finished after 852 timesteps\n",
      "Episode finished after 866 timesteps\n",
      "Episode finished after 894 timesteps\n",
      "Episode finished after 919 timesteps\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "block_reward = 0.0;\n",
    "total_epochs = 0;\n",
    "\n",
    "\n",
    "#block_reward = np.zeros((16), dtype=float);\n",
    "#total_epochs = np.zeros((16), dtype=float);\n",
    "   \n",
    "# Create estimators\n",
    "q_estimator = Estimator(nfeats*nwindow, nhidden, nactions)\n",
    "target_estimator = Estimator(nfeats*nwindow, nhidden, nactions)\n",
    "\n",
    "# The epsilon and learning rate decay schedules\n",
    "epsilons = np.linspace(epsilon_start, epsilon_end, neps)\n",
    "learning_rates = np.linspace(learning_rate, lr_end, nlr)\n",
    "\n",
    "# Initialize the games\n",
    "print(\"Initializing games...\"); sys.stdout.flush()\n",
    "envs = np.empty(npar, dtype=object)\n",
    "state = np.zeros([nfeats * nwindow, npar], dtype=float)\n",
    "rewards = np.zeros([npar], dtype=float)\n",
    "dones = np.empty(npar, dtype=int)\n",
    "actions = np.zeros([npar], dtype=int)\n",
    "\n",
    "\n",
    "for i in range(npar):\n",
    "    envs[i] = gym.make(game_type)\n",
    "   \n",
    "    ##################################################################################\n",
    "    ##                                                                              ##\n",
    "    ## TODO: Advance each environment by a random number of steps, where the number ##\n",
    "    ##       of steps is sampled uniformly from [nwindow, init_moves].              ##\n",
    "    ##       Use random steps to advance.                                           ## \n",
    "    ##                                                                              ##\n",
    "    ## Update the total reward and total epochs variables as you go.                ##\n",
    "    ## If an environment returns done=True, reset it and increment the epoch count. ##\n",
    "    ##                                                                              ##\n",
    "    ##################################################################################\n",
    "    game_len = random.randint(nwindow, init_moves)\n",
    "    observation = envs[i].reset()\n",
    "    for t in range(game_len):\n",
    "        actions[i] = envs[i].action_space.sample()\n",
    "        obs_prev = observation\n",
    "        observation, rewards[i], dones[i], _ = envs[i].step(actions[i])\n",
    "        if dones[i]:\n",
    "            total_epochs += 1\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            rewards[i] = 0 \n",
    "            envs[i].reset()\n",
    "        block_reward += rewards[i]\n",
    "    state[:, i] = np.hstack([observation, float(1), obs_prev, float(1)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17044.0\n",
      "800\n",
      "21.305\n"
     ]
    }
   ],
   "source": [
    "print(block_reward)\n",
    "print(total_epochs)\n",
    "print(block_reward / total_epochs) #Est. avg. Reward - right now ~20 much lower than 195..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement Deep Q-Learning\n",
    "In this cell you actually implement the algorithm. We've given you comments to define all the steps. You should also add book-keeping steps to keep track of the loss, reward and number of epochs (where env.step() returns done = true). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, time 0.0, loss 0.00546103, epochs 0, reward/epoch 17060.00000\n",
      "step 1000, time 1.2, loss 5.59363770, epochs 736, reward/epoch 20.73913\n",
      "step 2000, time 2.4, loss 5.36341728, epochs 1507, reward/epoch 19.75227\n",
      "step 3000, time 3.4, loss 5.07639792, epochs 2364, reward/epoch 17.66978\n",
      "step 4000, time 4.4, loss 4.90933500, epochs 3338, reward/epoch 15.42710\n",
      "step 5000, time 5.4, loss 4.67515209, epochs 4439, reward/epoch 13.53224\n",
      "step 6000, time 6.4, loss 4.46887353, epochs 5693, reward/epoch 11.75917\n",
      "step 7000, time 7.4, loss 4.33094429, epochs 7054, reward/epoch 10.75606\n",
      "step 8000, time 8.4, loss 4.17656118, epochs 8517, reward/epoch 9.93643\n",
      "step 9000, time 9.4, loss 4.01258534, epochs 10093, reward/epoch 9.15228\n",
      "step 10000, time 10.6, loss 3.85280061, epochs 11752, reward/epoch 8.64436\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "block_loss = 0.0\n",
    "last_epochs=0\n",
    "total_epochs = 0 \n",
    "\n",
    "for istep in np.arange(nsteps): \n",
    "    if (render): envs[0].render()\n",
    "  \n",
    "    #########################################################################\n",
    "    ## TODO: Implement Q-Learning                                          ##\n",
    "    ##                                                                     ##\n",
    "    ## At high level, your code should:                                    ##\n",
    "    ## * Update epsilon and learning rate.                                 ##\n",
    "    ## * Update target estimator from Q-estimator if needed.               ##\n",
    "    ## * Get the next action probabilities for the minibatch by running    ##\n",
    "    ##   the policy on the current state with the Q-estimator.             ##\n",
    "    ## * Then for each environment:                                        ##\n",
    "    ##     ** Pick an action according to the action probabilities.        ##\n",
    "    ##     ** Step in the gym with that action.                            ##\n",
    "    ##     ** Process the observation and concat it to the last nwindow-1  ##\n",
    "    ##        processed observations to form a new state.                  ##\n",
    "    ## Then for all environments (vectorized):                             ##\n",
    "    ## * Predict Q-scores for the new state using the target estimator.    ##\n",
    "    ## * Compute new expected rewards using those Q-scores.                ##\n",
    "    ## * Using those expected rewards as a target, compute gradients and   ##\n",
    "    ##   update the Q-estimator.                                           ##\n",
    "    ## * Step to the new state.                                            ##\n",
    "    ##                                                                     ##\n",
    "    #########################################################################\n",
    "    \n",
    "    # Update epsilon and learning rate \n",
    "    current_eps_index = int(istep / (nsteps / (1.0*neps)))\n",
    "    current_eps = epsilons[current_eps_index]\n",
    "    current_lr = learning_rates[current_eps_index]\n",
    "    \n",
    "    # Check if need to update target-value function \n",
    "    update_estimator(q_estimator, target_estimator, nwindow, istep)\n",
    "    \n",
    "    next_action_probs = policy(q_estimator, state, current_eps)\n",
    "    chosen_actions = []\n",
    "    rewards_recived = []\n",
    "    dones = []\n",
    "    for i in range(npar):\n",
    "        action_prob = next_action_probs[:, i]\n",
    "        binary_vec = np.random.multinomial(1, action_prob) #All 0s except 1 in chosen action \n",
    "        rand_action = np.argmax(binary_vec)\n",
    "        chosen_actions.append(rand_action)\n",
    "        state[5:9, i] = state[0:4, i]\n",
    "        state[0:4, i], reward, done, _ = envs[i].step(rand_action)\n",
    "        dones.append(done)\n",
    "        if done:\n",
    "            reward = 0\n",
    "            total_epochs += 1 # Another game has ended. Add to global count\n",
    "            envs[i].reset()\n",
    "        rewards_recived.append(reward)\n",
    "        block_reward += reward \n",
    "    \n",
    "    chosen_actions = np.array(chosen_actions)\n",
    "    rewards_recived = np.array(rewards_recived)\n",
    "    dones = np.array(dones)\n",
    "    \n",
    "    pred_scores = target_estimator.predict(state)\n",
    "    expected_rewards = rewards_recived + discount_factor * np.max(pred_scores, axis=0) #Bellman eqn  \n",
    "    loss = q_estimator.gradient(state, chosen_actions, expected_rewards) #Updates gradient, returns loss \n",
    "    q_estimator.rmsprop(current_lr, decay_rate)\n",
    "    block_loss += loss\n",
    "        \n",
    "    t = time.time() - t0\n",
    "    if (istep % printsteps == 0): \n",
    "        print(\"step {:0d}, time {:.1f}, loss {:.8f}, epochs {:0d}, reward/epoch {:.5f}\".format(\n",
    "                istep, t, block_loss/printsteps, total_epochs, block_reward/np.maximum(1,total_epochs-last_epochs)))\n",
    "        last_epochs = total_epochs\n",
    "        block_reward = 0.0\n",
    "        block_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the model now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(q_estimator.model, open(\"cartpole_q_estimator.p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can reload the model later if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_estimator = Estimator(nfeats*nwindow, nhidden, nactions)\n",
    "test_estimator.model = pickle.load(open(\"cartpole_q_estimator.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And animate the model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state0 = state[:,0]\n",
    "for i in np.arange(200):\n",
    "    envs[0].render()\n",
    "    preds = test_estimator.predict(state0)\n",
    "    iaction = np.argmax(preds)\n",
    "    obs, _, done0, _ = envs[0].step(VALID_ACTIONS[iaction])\n",
    "    state0 = np.concatenate((state0[nfeats:], preprocess(obs)))\n",
    "    if (done0): envs[0].reset()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there we have it. Simple 1-step Q-Learning can solve easy problems very fast. Note that environments that produce images will be much slower to train on than environments (like CartPole) which return an observation of the state of the system. But this model can still train on those image-based games - like Atari games. It will take hours-days however. It you try training on visual environments, we recommend you run the most expensive step - rmsprop - less often (e.g. every 10 iterations). This gives about a 3x speedup. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optional\n",
    "Do **one** of the following tasks:\n",
    "* Adapt the DQN algorithm to another environment - it can use direct state observations.  Call <code>env.get_action_meanings()</code> to find out what actions are allowed. Summarize training performance: your final average reward/epoch, the number of steps required to train, and any modifications to the model or its parameters that you made.\n",
    "* Try smarter schedules for epsilon and learning rate. Rewards for CartPole increase very sharply (several orders of magnitude) with better policies, especially as epsilon --> 0. Gradients will also change drastically, so the initial learning rate is probably not good later on. Try schedules for decreasing epsilon that allow the model to better adapt. Try other learning rate schedules, or setting learning rate based on average reward. \n",
    "* Try a fancier model. e.g. add another hidden layer, or try sigmoid non-linearities."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
